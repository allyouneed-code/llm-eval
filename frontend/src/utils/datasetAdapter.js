/**
 * src/utils/datasetAdapter.js
 * ä¿®å¤ç‰ˆï¼šä¿ç•™äº†æ—§çš„å¸¸é‡å¯¼å‡ºä»¥é˜²æ­¢ Import Errorï¼ŒåŒæ—¶å†…ç½®äº†è‡ªåŠ¨åå¤„ç†é€»è¾‘
 */

// ==========================================
// 1. æ ¸å¿ƒå®šä¹‰
// ==========================================

export const TASK_TYPES = {
  CHOICE: { 
    label: 'å®¢è§‚é€‰æ‹©é¢˜ (Multiple Choice)', 
    value: 'choice',
    desc: 'é€‚ç”¨äº MMLU, CMMLU, ARC ç­‰æœ‰æ ‡å‡†é€‰é¡¹(A/B/C/D)çš„é¢˜ç›®'
  },
  QA: { 
    label: 'å¼€æ”¾å¼é—®ç­” (Open QA)', 
    value: 'qa',
    desc: 'é€‚ç”¨äºç¿»è¯‘ã€æ‘˜è¦ã€ç®€ç­”ç­‰ç”Ÿæˆå¼ä»»åŠ¡'
  }
}

export const TASK_SLOTS = {
  [TASK_TYPES.CHOICE.value]: [
    { key: 'question', label: 'é¢˜ç›® (Question)', required: true },
    { key: 'optA', label: 'é€‰é¡¹ A', required: true },
    { key: 'optB', label: 'é€‰é¡¹ B', required: true },
    { key: 'optC', label: 'é€‰é¡¹ C', required: false },
    { key: 'optD', label: 'é€‰é¡¹ D', required: false },
    { key: 'answer', label: 'æ ‡å‡†ç­”æ¡ˆ (Key)', required: true }
  ],
  [TASK_TYPES.QA.value]: [
    { key: 'prompt', label: 'è¾“å…¥/æç¤ºè¯ (Input)', required: true },
    { key: 'target', label: 'å‚è€ƒç­”æ¡ˆ (Target)', required: true }
  ]
}

export const TASK_METRICS = {
  [TASK_TYPES.CHOICE.value]: [
    { label: 'Accuracy (å‡†ç¡®ç‡)', value: 'Accuracy', default: true },
    { label: 'F1 Score (åŠ æƒå¾—åˆ†)', value: 'F1', default: false }
  ],
  [TASK_TYPES.QA.value]: [
    { label: 'ROUGE-L (æ–‡æœ¬ç›¸ä¼¼åº¦)', value: 'ROUGE', default: true },
    { label: 'BLEU-4 (æœºå™¨ç¿»è¯‘æ ‡å‡†)', value: 'BLEU', default: false },
    { label: 'Exact Match (å®Œå…¨åŒ¹é…)', value: 'EM', default: false }
  ]
}

// â¬‡ï¸ å…¼å®¹æ€§ä¿®å¤ï¼šä¿ç•™è¿™äº›å¯¼å‡ºï¼Œé˜²æ­¢ Vue ç»„ä»¶æŠ¥é”™
export const CHOICE_POST_PROCESSORS = [
  { label: 'æå–é¦–é€‰é¡¹ (A/B/C/D)', value: 'first_option' }
]
export const QA_POST_PROCESSORS = []

// ==========================================
// 2. è‡ªåŠ¨æ˜ å°„é€»è¾‘ (Auto-Mapping)
// ==========================================

const METRIC_EVALUATOR_MAP = {
  'Accuracy': 'AccEvaluator',
  'F1': 'F1Evaluator',
  'ROUGE': 'RougeEvaluator',
  'BLEU': 'BleuEvaluator',
  'EM': 'EMEvaluator'
}

/**
 * æ ¹æ®æŒ‡æ ‡å’Œä»»åŠ¡ç±»å‹ï¼Œè·å–æœ€ä½³åå¤„ç†é…ç½®
 */
function getAutoPostProcessCfg(metric, taskType) {
  // åœºæ™¯ A: é€‰æ‹©é¢˜ (Choice) -> è‡ªåŠ¨åº”ç”¨ first_option_postprocess
  if (taskType === TASK_TYPES.CHOICE.value) {
    if (metric === 'Accuracy' || metric === 'F1') {
      return { 
        type: 'opencompass.utils.text_postprocessors.first_option_postprocess',
        options: 'ABCD'
      }
    }
  }

  // åœºæ™¯ B: é—®ç­”é¢˜ (QA) -> é»˜è®¤ null (Raw Match)
  // ROUGE/BLEU åœ¨é€šç”¨åœºæ™¯ä¸‹ä¸éœ€è¦ç‰¹å®šåå¤„ç†
  return null
}

function generatePromptTemplate(taskType, mapping) {
  if (taskType === TASK_TYPES.CHOICE.value) {
    // ç¡®ä¿è¿™é‡Œçš„ mapping key å¯¹åº”çš„æ˜¯å®é™…çš„ CSV åˆ—å
    // æ³¨æ„ï¼šæ¨¡æ¿é‡Œç”¨ {MappingKey}ï¼Œè€Œä¸æ˜¯ {SlotKey}
    let template = `Question: {${mapping.question}}\n`
    if (mapping.optA) template += `A. {${mapping.optA}}\n`
    if (mapping.optB) template += `B. {${mapping.optB}}\n`
    if (mapping.optC) template += `C. {${mapping.optC}}\n`
    if (mapping.optD) template += `D. {${mapping.optD}}\n`
    template += `Answer:`
    return template
  }
  if (taskType === TASK_TYPES.QA.value) {
    return `Question: {${mapping.prompt}}\nAnswer:`
  }
  return ''
}

// ==========================================
// 3. å·¥å‚æ–¹æ³•
// ==========================================

export function generateConfigPayload(importState) {
  const { meta, taskType, columnMapping, metrics } = importState
  
  // 1. Reader Config
  const inputColumns = Object.values(columnMapping).filter(v => v)
  const outputColumnKey = taskType === TASK_TYPES.CHOICE.value ? 'answer' : 'target'
  const outputColumn = columnMapping[outputColumnKey]

  const readerCfg = {
    input_columns: inputColumns,
    output_column: outputColumn,
    // ğŸŒŸ æ ¸å¿ƒä¿®å¤ï¼šå¿…é¡»åŒ…å« mapping å­—æ®µï¼Œå¦åˆ™åç«¯ Schema æ ¡éªŒä¼šå¤±è´¥ (400 Bad Request)
    mapping: columnMapping 
  }

  // 2. Infer Config
  const promptTemplateStr = generatePromptTemplate(taskType, columnMapping)
  const inferCfg = {
    prompt_template: {
      type: 'PromptTemplate',
      template: promptTemplateStr
    },
    retriever: { type: 'ZeroRetriever' },
    inferencer: { type: 'GenInferencer' }
  }

  // 3. Generate Configs List
  const configs = metrics.map((metricName) => {
    // A. ç¡®å®š Evaluator
    const evaluatorType = METRIC_EVALUATOR_MAP[metricName] || 'AccEvaluator'
    const evaluatorConfig = { type: evaluatorType }

    // B. è‡ªåŠ¨ç¡®å®š PostProcess (å¿½ç•¥å‰ç«¯ä¼ æ¥çš„ postProcess å­—æ®µ)
    const postProcessCfg = getAutoPostProcessCfg(metricName, taskType) || {}
    
    return {
      config_name: `${meta.name}_${metricName}`,
      mode: 'gen', 
      display_metric: metricName,
      
      reader_cfg: JSON.stringify(readerCfg),
      infer_cfg: JSON.stringify(inferCfg),
      metric_config: JSON.stringify({ evaluator: evaluatorConfig }),
      post_process_cfg: JSON.stringify(postProcessCfg),
      few_shot_cfg: JSON.stringify({})
    }
  })

  return configs
}