import os
import shutil
import json
import pandas as pd
from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form
from fastapi.responses import FileResponse
from sqlmodel import Session, select, func, or_
from sqlalchemy.orm import selectinload 
from typing import List, Optional, Dict, Any

from app.core.database import get_session
from app.models.dataset import DatasetMeta, DatasetConfig
from app.schemas.dataset_schema import (
    DatasetMetaRead, DatasetConfigCreate, 
    DatasetPaginationResponse, CategoryStat
)

router = APIRouter()

UPLOAD_DIR = "data/datasets"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# ==========================================
# 1. æ ¸å¿ƒå·¥å…·ï¼šæ•°æ®æ‰å¹³åŒ– (Flatten Logic)
# ==========================================

def _flatten_row(row: Dict[str, Any], parent_key: str = '', sep: str = '_') -> Dict[str, Any]:
    """
    é€’å½’æ‰å¹³åŒ– JSON è¡Œï¼Œå¹¶æ™ºèƒ½å¤„ç† choices åˆ—è¡¨
    ä¾‹å¦‚: 
    input: { "question": { "stem": "Q1", "choices": [{"label": "A", "text": "Apple"}] } }
    output: { "question_stem": "Q1", "question_choices_A": "Apple" }
    """
    items = {}
    for k, v in row.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        
        if isinstance(v, dict):
            # é€’å½’å¤„ç†å­—å…¸
            items.update(_flatten_row(v, new_key, sep=sep))
            
        elif isinstance(v, list):
            # ğŸŒŸ æ™ºèƒ½å¤„ç†åˆ—è¡¨ï¼šå°è¯•è¯†åˆ«ä¸ºé€‰é¡¹åˆ—è¡¨
            is_choice_list = False
            extracted = {}
            
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆ [{"label": "A", "text": "..."}] æˆ–ç±»ä¼¼ç»“æ„
            # ä»…å½“åˆ—è¡¨éç©ºä¸”å…ƒç´ ä¸ºå­—å…¸æ—¶æ£€æŸ¥
            if v and isinstance(v[0], dict):
                # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„ key
                first_keys = v[0].keys()
                # å¸¸è§çš„ label key
                label_key = next((lk for lk in ['label', 'key', 'option'] if lk in first_keys), None)
                # å¸¸è§çš„ content key
                text_key = next((tk for tk in ['text', 'content', 'value'] if tk in first_keys), None)
                
                if label_key and text_key:
                    is_choice_list = True
                    for item in v:
                        if label_key in item and text_key in item:
                            label_val = item[label_key]
                            # ç”Ÿæˆåˆ—åï¼Œå¦‚ question_choices_A
                            col_name = f"{new_key}{sep}{label_val}"
                            extracted[col_name] = item[text_key]
            
            if is_choice_list:
                items.update(extracted)
            else:
                # å¦‚æœä¸æ˜¯æ ‡å‡†é€‰é¡¹åˆ—è¡¨ï¼Œä¿ç•™åŸæ · (è½¬å­—ç¬¦ä¸²æˆ–ä¿ç•™å¯¹è±¡)
                # ä¸ºäº†å…¼å®¹ Pandas/CSVï¼Œé€šå¸¸è½¬ä¸º JSON å­—ç¬¦ä¸²æ›´å®‰å…¨ï¼Œä½†è¿™é‡Œæš‚ä¿ç•™åŸå€¼
                items[new_key] = v
        else:
            items[new_key] = v
            
    return items

def _process_and_save_file(upload_file: UploadFile, save_path: str):
    """
    è¯»å–ä¸Šä¼ æ–‡ä»¶ï¼Œæ‰§è¡Œæ‰å¹³åŒ–å¤„ç†ï¼Œå¹¶ä¿å­˜åˆ°ç£ç›˜
    """
    filename = upload_file.filename.lower()
    
    # ä»…é’ˆå¯¹ JSONL/JSON è¿›è¡Œé«˜çº§å¤„ç†
    if filename.endswith(".jsonl") or filename.endswith(".json"):
        rows = []
        try:
            # è¯»å–å†…å®¹
            content = upload_file.file.read()
            # é‡ç½®æŒ‡é’ˆä»¥ä¾¿åç»­å¯èƒ½çš„æ“ä½œ (è™½ç„¶è¿™é‡Œè¯»å®Œå°±å¤„ç†äº†)
            upload_file.file.seek(0)
            
            # è§£æ
            if filename.endswith(".jsonl"):
                # JSONL: é€è¡Œè§£æ
                lines = content.decode('utf-8').splitlines()
                for line in lines:
                    if line.strip():
                        rows.append(json.loads(line))
            else:
                # JSON: æ•´ä½“è§£æ
                data = json.loads(content)
                if isinstance(data, list):
                    rows = data
                else:
                    rows = [data]
            
            # æ‰§è¡Œæ‰å¹³åŒ–
            flattened_rows = [_flatten_row(row) for row in rows]
            
            # è½¬æ¢ä¸º DataFrame å¹¶ä¿å­˜ä¸º JSONL (æ ‡å‡†åŒ–æ ¼å¼)
            # å³ä½¿åŸæ–‡ä»¶æ˜¯ JSONï¼Œæˆ‘ä»¬ä¹Ÿå­˜ä¸º JSONLï¼Œå› ä¸º OpenCompass å¯¹ JSONL æ”¯æŒæœ€å¥½
            df = pd.DataFrame(flattened_rows)
            
            # å¼ºåˆ¶è½¬æ¢ä¸º jsonl æ ¼å¼ä¿å­˜ï¼Œè¦†ç›–åŸå§‹åç¼€é€»è¾‘
            # ä½†ä¸ºäº†ä¿æŒ save_path çš„æ‰©å±•åä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è¿™é‡Œå¦‚æœ save_path æ˜¯ .jsonï¼Œä¹Ÿå†™æˆ json æ ¼å¼
            # å»ºè®®ï¼šç»Ÿä¸€å†…éƒ¨å­˜å‚¨ä¸º .jsonl æ ¼å¼æ›´ä¼˜ï¼Œä½†ä¸ºäº†é€»è¾‘ç®€å•ï¼Œæˆ‘ä»¬æŒ‰æ‰©å±•åè¾“å‡º
            
            if save_path.endswith(".jsonl"):
                df.to_json(save_path, orient='records', lines=True, force_ascii=False)
            else:
                df.to_json(save_path, orient='records', force_ascii=False)
                
        except Exception as e:
            print(f"Flattening failed: {e}, falling back to raw copy")
            # å¦‚æœè§£æå¤±è´¥ï¼Œå›é€€åˆ°ç›´æ¥æ‹·è´
            upload_file.file.seek(0)
            with open(save_path, "wb") as buffer:
                shutil.copyfileobj(upload_file.file, buffer)
    else:
        # CSV/Excel ç›´æ¥æ‹·è´ï¼Œä¸åšå¤„ç†
        upload_file.file.seek(0)
        with open(save_path, "wb") as buffer:
            shutil.copyfileobj(upload_file.file, buffer)

def _parse_preview_data(filepath_or_buffer, filename: str):
    """è§£ææ–‡ä»¶å‰å‡ è¡Œç”¨äºé¢„è§ˆ (åº”ç”¨æ‰å¹³åŒ–é€»è¾‘)"""
    filename = filename.lower()
    df = None
    try:
        # å¦‚æœæ˜¯ä¸Šä¼ å¯¹è±¡ (UploadFile.file)ï¼Œè¯»å–å†…å®¹å¹¶è§£æ
        # è¿™é‡Œçš„ filepath_or_buffer å¯èƒ½æ˜¯ bytes IOï¼Œä¹Ÿå¯èƒ½æ˜¯è·¯å¾„å­—ç¬¦ä¸²
        
        is_path = isinstance(filepath_or_buffer, str)
        
        if filename.endswith(".jsonl") or filename.endswith(".json"):
            # é’ˆå¯¹ JSON/JSONLï¼Œå…ˆæ‰‹åŠ¨è¯»å–å‰å‡ è¡Œè¿›è¡Œæ‰å¹³åŒ–ï¼Œè€Œä¸æ˜¯ç›´æ¥ç”¨ pd.read_json
            rows = []
            if is_path:
                # è¯»æœ¬åœ°æ–‡ä»¶ (é¢„è§ˆå·²ä¿å­˜çš„)
                with open(filepath_or_buffer, 'r', encoding='utf-8') as f:
                    if filename.endswith(".jsonl"):
                        for _ in range(5):
                            line = f.readline()
                            if not line: break
                            rows.append(json.loads(line))
                    else:
                        # JSON åªèƒ½å…¨è¯» (æˆ–è€…è¯»ä¸€éƒ¨åˆ†ä½†å¾ˆéš¾æ§åˆ¶ç»“æ„)ï¼Œè¿™é‡Œå‡è®¾æ–‡ä»¶ä¸å¤§æˆ–åªé¢„è§ˆå·²ä¿å­˜çš„
                        data = json.load(f)
                        rows = data[:5] if isinstance(data, list) else [data]
            else:
                # è¯»å†…å­˜æµ (ä¸Šä¼ æ—¶çš„é¢„è§ˆ)
                # æ³¨æ„ï¼šæµåªèƒ½è¯»ä¸€æ¬¡ï¼Œè¯»å®Œè¦ seek å›å»ï¼Œæˆ–è€…åªè¯»ä¸€éƒ¨åˆ†
                # è¿™é‡Œç®€å•å¤„ç†ï¼šè¯»å–å‰ 5 è¡Œ (é’ˆå¯¹ JSONL)
                if filename.endswith(".jsonl"):
                    for _ in range(5):
                        line = filepath_or_buffer.readline()
                        if not line: break
                        rows.append(json.loads(line))
                    filepath_or_buffer.seek(0) # é‡ç½®
                else:
                    # JSON æµï¼Œå…¨è¯»
                    content = filepath_or_buffer.read()
                    filepath_or_buffer.seek(0)
                    data = json.loads(content)
                    rows = data[:5] if isinstance(data, list) else [data]
            
            # æ‰å¹³åŒ–é¢„è§ˆæ•°æ®
            flat_rows = [_flatten_row(row) for row in rows]
            df = pd.DataFrame(flat_rows)
            
        elif filename.endswith(".csv"):
            df = pd.read_csv(filepath_or_buffer, nrows=5, on_bad_lines='skip')
        elif filename.endswith(".xlsx") or filename.endswith(".xls"):
            df = pd.read_excel(filepath_or_buffer, nrows=5)
        
        if df is not None:
            # å¤„ç† NaN
            df = df.where(pd.notnull(df), None)
            return {
                "columns": list(df.columns),
                "rows": df.to_dict(orient="records")
            }
    except Exception as e:
        print(f"Parse Error: {e}")
    return {"columns": [], "rows": []}

def _extract_metric_name(eval_cfg_json: str, default: str = "Accuracy") -> str:
    """ä» metric_config JSON ä¸­æå–ä¸»è¦çš„æŒ‡æ ‡åç§°"""
    try:
        data = json.loads(eval_cfg_json)
        evaluator = data.get('evaluator', {})
        etype = evaluator.get('type') if isinstance(evaluator, dict) else evaluator
        
        s_type = str(etype)
        if 'AccEvaluator' in s_type: return 'Accuracy'
        if 'BleuEvaluator' in s_type: return 'BLEU'
        if 'RougeEvaluator' in s_type: return 'ROUGE'
        return default
    except:
        return default

# ==========================================
# 2. é¢„è§ˆä¸ä¸‹è½½æ¥å£
# ==========================================

@router.post("/preview")
def preview_dataset(file: UploadFile = File(...)):
    # ç›´æ¥ä½¿ç”¨ file.file (SpooledTemporaryFile)
    return _parse_preview_data(file.file, file.filename)

@router.get("/{meta_id}/preview")
def preview_saved_dataset(meta_id: int, session: Session = Depends(get_session)):
    meta = session.get(DatasetMeta, meta_id)
    if not meta or meta.is_deleted or not meta.configs:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°ç›¸å…³æ•°æ®æ–‡ä»¶")
    
    config = meta.configs[0]
    if not os.path.exists(config.file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶åœ¨ç£ç›˜ä¸Šä¸å­˜åœ¨")
    
    return _parse_preview_data(config.file_path, config.file_path)

@router.get("/{meta_id}/download")
def download_dataset_file(meta_id: int, session: Session = Depends(get_session)):
    meta = session.get(DatasetMeta, meta_id)
    if not meta or meta.is_deleted or not meta.configs:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ–‡ä»¶")
    
    config = meta.configs[0]
    if not os.path.exists(config.file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    filename = os.path.basename(config.file_path)
    return FileResponse(path=config.file_path, filename=filename, media_type='application/octet-stream')

# ==========================================
# 3. æ ¸å¿ƒæ¥å£ï¼šåˆ›å»ºä¸è¯»å–
# ==========================================

@router.get("/stats", response_model=List[CategoryStat])
def get_dataset_stats(session: Session = Depends(get_session)):
    statement = select(DatasetMeta.category, func.count(DatasetMeta.id))\
        .where(DatasetMeta.is_deleted == False)\
        .group_by(DatasetMeta.category)
    results = session.exec(statement).all()
    stats = [{"category": row[0], "count": row[1]} for row in results]
    return stats

@router.post("/", response_model=DatasetMetaRead)
def create_dataset(
    name: str = Form(...),
    category: str = Form(...),
    description: Optional[str] = Form(None),
    configs_json: str = Form(...), 
    file: UploadFile = File(...),
    session: Session = Depends(get_session)
):
    # 1. æ£€æŸ¥æˆ–åˆ›å»ºå…ƒæ•°æ®
    statement = select(DatasetMeta).where(DatasetMeta.name == name)
    meta = session.exec(statement).first()
    
    if not meta:
        meta = DatasetMeta(name=name, category=category, description=description)
        session.add(meta)
        session.commit()
        session.refresh(meta)
    else:
        if meta.is_deleted:
            meta.is_deleted = False
            meta.category = category
            if description: meta.description = description
            session.add(meta)
            session.commit()
            session.refresh(meta)
    
    # 2. ä¿å­˜å¹¶å¤„ç†æ–‡ä»¶ (ETL)
    file_ext = os.path.splitext(file.filename)[1].lower()
    # å¼ºåˆ¶ç»Ÿä¸€ä½¿ç”¨ jsonl ä½œä¸ºå­˜å‚¨æ ¼å¼ (å¦‚æœåŸæ–‡ä»¶æ˜¯ JSON/JSONL)
    if file_ext in ['.json', '.jsonl']:
        save_name = f"{name}_base.jsonl"
    else:
        save_name = f"{name}_base{file_ext}"
        
    save_path = os.path.join(UPLOAD_DIR, save_name)
    abs_path = os.path.abspath(save_path)
    
    # ğŸŒŸ è°ƒç”¨å¤„ç†å‡½æ•°ï¼šä¿å­˜å¹¶æ‰å¹³åŒ–
    _process_and_save_file(file, save_path)
        
    # 3. è§£æå¹¶æ‰¹é‡å¤„ç†é…ç½®
    try:
        configs_list = json.loads(configs_json)
        if not isinstance(configs_list, list):
            raise ValueError("configs_json must be a list")
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"é…ç½®æ ¼å¼é”™è¯¯: {str(e)}")

    processed_count = 0
    errors = []

    for cfg_data in configs_list:
        try:
            cfg_data["meta_id"] = meta.id
            cfg_data["file_path"] = abs_path
            
            if not cfg_data.get("config_name"):
                mode_suffix = cfg_data.get("mode", "gen")
                cfg_data["config_name"] = f"{name}_{mode_suffix}"
                
            if not cfg_data.get("display_metric"):
                cfg_data["display_metric"] = _extract_metric_name(cfg_data.get("metric_config", "{}"))

            validated_config = DatasetConfigCreate(**cfg_data)
            
            existing = next((c for c in meta.configs if c.config_name == validated_config.config_name), None)
            
            if existing:
                existing.mode = validated_config.mode
                existing.file_path = validated_config.file_path
                existing.reader_cfg = validated_config.reader_cfg
                existing.infer_cfg = validated_config.infer_cfg
                existing.metric_config = validated_config.metric_config
                existing.display_metric = validated_config.display_metric
                existing.post_process_cfg = validated_config.post_process_cfg
                existing.few_shot_cfg = validated_config.few_shot_cfg
                session.add(existing)
            else:
                db_config = DatasetConfig(**validated_config.model_dump())
                session.add(db_config)
            
            processed_count += 1
            
        except Exception as e:
            errors.append(f"Config '{cfg_data.get('config_name', 'unknown')}': {str(e)}")
            continue

    if processed_count == 0 and errors:
        raise HTTPException(status_code=400, detail=f"å¯¼å…¥å¤±è´¥: {errors[0]}")

    session.commit()
    session.refresh(meta)
    return meta

@router.get("/", response_model=DatasetPaginationResponse)
def read_datasets(
    session: Session = Depends(get_session),
    page: int = 1,
    page_size: int = 10,
    category: Optional[str] = None,
    keyword: Optional[str] = None,
    private_only: bool = False
):
    offset = (page - 1) * page_size
    query = select(DatasetMeta).where(DatasetMeta.is_deleted == False)
    
    if category and category != 'All':
        query = query.where(DatasetMeta.category == category)
    
    if keyword:
        query = query.where(or_(DatasetMeta.name.contains(keyword), DatasetMeta.description.contains(keyword)))
    
    if private_only:
        query = query.join(DatasetConfig).where(DatasetConfig.file_path.not_like("official://%"))
        
    count_statement = select(func.count()).select_from(query.subquery())
    total = session.exec(count_statement).one()
    
    query = query.options(selectinload(DatasetMeta.configs))
    query = query.offset(offset).limit(page_size)
    items = session.exec(query).unique().all()
    
    return DatasetPaginationResponse(total=total, items=items)

@router.delete("/{meta_id}")
def delete_dataset(meta_id: int, session: Session = Depends(get_session)):
    meta = session.get(DatasetMeta, meta_id)
    if not meta:
        raise HTTPException(status_code=404, detail="Dataset not found")
    
    # ==========================================
    # ğŸ†• æ–°å¢é€»è¾‘ï¼šåˆ é™¤å…³è”çš„ç‰©ç†æ–‡ä»¶
    # ==========================================
    files_to_delete = set()
    
    # 1. æ”¶é›†è¯¥æ•°æ®é›†å…³è”çš„æ‰€æœ‰å”¯ä¸€æ–‡ä»¶è·¯å¾„
    if meta.configs:
        for config in meta.configs:
            path = config.file_path
            # ç¡®ä¿è·¯å¾„å­˜åœ¨ï¼Œä¸”ä¸æ˜¯ 'official://' ç­‰ç‰¹æ®Šæ ‡è¯†
            if path and isinstance(path, str) and not path.startswith("official://"):
                files_to_delete.add(path)
    
    # 2. æ‰§è¡Œç‰©ç†åˆ é™¤
    for file_path in files_to_delete:
        try:
            if os.path.exists(file_path) and os.path.isfile(file_path):
                os.remove(file_path)
                print(f"[Delete] å·²åˆ é™¤æ–‡ä»¶: {file_path}")
        except Exception as e:
            # æ‰“å°é”™è¯¯ä½†ä¸é˜»æ–­æµç¨‹ï¼Œé˜²æ­¢å› æ–‡ä»¶æƒé™é—®é¢˜å¯¼è‡´æ— æ³•åˆ é™¤æ•°æ®åº“è®°å½•
            print(f"[Warning] åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    # ==========================================
    
    # 3. æ•°æ®åº“å±‚é¢å¤„ç† (ä¿æŒè½¯åˆ é™¤æˆ–æ”¹ä¸ºç¡¬åˆ é™¤)
    # æ—¢ç„¶æ–‡ä»¶éƒ½åˆ äº†ï¼Œé€šå¸¸å»ºè®®è¿™é‡Œä¹Ÿå¯ä»¥è€ƒè™‘ç›´æ¥ç¡¬åˆ é™¤ï¼šsession.delete(meta)
    # ä½†ä¸ºäº†ä¿æŒä¸ create_dataset ä¸­â€œåŒåå¤æ´»â€é€»è¾‘å…¼å®¹ï¼Œç›®å‰ç»´æŒè½¯åˆ é™¤é€»è¾‘æ˜¯å®‰å…¨çš„ã€‚
    meta.is_deleted = True
    session.add(meta)
    session.commit()
    
    return {"ok": True, "detail": "Dataset deleted and associated files removed"}

@router.get("/configs")
def get_all_dataset_configs(session: Session = Depends(get_session)):
    configs = session.exec(select(DatasetConfig)).all()
    return configs