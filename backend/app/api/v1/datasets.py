import os
import shutil
import json
import pandas as pd
from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form
from fastapi.responses import FileResponse
from sqlmodel import Session, select
from typing import List, Optional

from app.core.database import get_session
# å¼•å…¥æ–°æ¨¡å‹
from app.models.dataset import DatasetMeta, DatasetConfig
# å¼•å…¥æ–° Schema (åŒ…å« DatasetConfigCreate æ ¡éªŒé€»è¾‘)
from app.schemas.dataset_schema import DatasetMetaRead, DatasetConfigCreate

router = APIRouter()

UPLOAD_DIR = "data/datasets"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# ==========================================
# 1. è¾…åŠ©å‡½æ•°
# ==========================================

def _parse_preview_data(filepath_or_buffer, filename: str):
    """è§£ææ–‡ä»¶å‰å‡ è¡Œç”¨äºé¢„è§ˆ"""
    filename = filename.lower()
    df = None
    try:
        if filename.endswith(".csv"):
            df = pd.read_csv(filepath_or_buffer, nrows=5, on_bad_lines='skip')
        elif filename.endswith(".json"):
            df = pd.read_json(filepath_or_buffer)
            df = df.head(5)
        elif filename.endswith(".jsonl"):
            with pd.read_json(filepath_or_buffer, lines=True, chunksize=5) as reader:
                for chunk in reader:
                    df = chunk
                    break
        elif filename.endswith(".xlsx") or filename.endswith(".xls"):
            df = pd.read_excel(filepath_or_buffer, nrows=5)
        
        if df is not None:
            df = df.where(pd.notnull(df), None)
            return {
                "columns": list(df.columns),
                "rows": df.to_dict(orient="records")
            }
    except Exception as e:
        print(f"Parse Error: {e}")
    return {"columns": [], "rows": []}

def _extract_metric_name(eval_cfg_json: str, default: str = "Accuracy") -> str:
    """ä» metric_config JSON ä¸­æå–ä¸»è¦çš„æŒ‡æ ‡åç§°"""
    try:
        data = json.loads(eval_cfg_json)
        # å…¼å®¹æ–‡æ¡£ä¸­çš„ç»“æ„: evaluator -> type æˆ– evaluator: "AccEvaluator"
        evaluator = data.get('evaluator', {})
        etype = evaluator.get('type') if isinstance(evaluator, dict) else evaluator
        
        # ç®€å•çš„æ˜ å°„è¡¨
        s_type = str(etype)
        if 'AccEvaluator' in s_type: return 'Accuracy'
        if 'BleuEvaluator' in s_type: return 'BLEU'
        if 'RougeEvaluator' in s_type: return 'ROUGE'
        if 'ToxicEvaluator' in s_type: return 'Toxicity'
        if 'Pass' in s_type or 'Code' in s_type: return 'Pass@k'
        return default
    except:
        return default

# ==========================================
# 2. é¢„è§ˆä¸ä¸‹è½½æ¥å£
# ==========================================

@router.post("/preview")
def preview_dataset(file: UploadFile = File(...)):
    return _parse_preview_data(file.file, file.filename)

@router.get("/{meta_id}/preview")
def preview_saved_dataset(meta_id: int, session: Session = Depends(get_session)):
    """é¢„è§ˆæ•°æ®é›†ï¼šé»˜è®¤é¢„è§ˆè¯¥æ•°æ®é›†ä¸‹çš„ç¬¬ä¸€ä¸ªé…ç½®å¯¹åº”çš„æ–‡ä»¶"""
    meta = session.get(DatasetMeta, meta_id)
    if not meta or not meta.configs:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°ç›¸å…³æ•°æ®æ–‡ä»¶")
    
    config = meta.configs[0]
    if not os.path.exists(config.file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶åœ¨ç£ç›˜ä¸Šä¸å­˜åœ¨")
    
    return _parse_preview_data(config.file_path, config.file_path)

@router.get("/{meta_id}/download")
def download_dataset_file(meta_id: int, session: Session = Depends(get_session)):
    meta = session.get(DatasetMeta, meta_id)
    if not meta or not meta.configs:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ–‡ä»¶")
    
    config = meta.configs[0]
    if not os.path.exists(config.file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    filename = os.path.basename(config.file_path)
    return FileResponse(path=config.file_path, filename=filename, media_type='application/octet-stream')

# ==========================================
# 3. æ ¸å¿ƒæ¥å£ï¼šåˆ›å»ºä¸è¯»å–
# ==========================================

@router.post("/", response_model=DatasetMetaRead)
def create_dataset(
    name: str = Form(...),
    category: str = Form(...),
    description: Optional[str] = Form(None),
    
    # === é…ç½®ç›¸å…³å­—æ®µ ===
    mode: str = Form("gen"),
    # æ¥æ”¶å®Œæ•´çš„ JSON å­—ç¬¦ä¸²é…ç½®
    reader_cfg: str = Form('{"input_columns":["input"], "output_column":"target"}'), 
    infer_cfg: str = Form('{}'),
    metric_config: str = Form('{"evaluator": {"type": "AccEvaluator"}}'),
    
    file: UploadFile = File(...),
    session: Session = Depends(get_session)
):
    """
    åˆ›å»ºæ•°æ®é›† (Meta) + é»˜è®¤é…ç½® (Config) + ä¸Šä¼ æ–‡ä»¶
    æ­¤å¤„é›†æˆäº† Pydantic æ ¡éªŒé€»è¾‘
    """
    
    # 1. æ£€æŸ¥æˆ–åˆ›å»ºå…ƒæ•°æ® (DatasetMeta)
    statement = select(DatasetMeta).where(DatasetMeta.name == name)
    meta = session.exec(statement).first()
    
    if not meta:
        meta = DatasetMeta(
            name=name,
            category=category,
            description=description
        )
        session.add(meta)
        session.commit()
        session.refresh(meta)
    
    # 2. ä¿å­˜æ–‡ä»¶ (ç‰©ç†å­˜å‚¨)
    file_ext = os.path.splitext(file.filename)[1]
    save_name = f"{name}_{mode}{file_ext}"
    save_path = os.path.join(UPLOAD_DIR, save_name)
    abs_path = os.path.abspath(save_path)
    
    file.file.seek(0)
    with open(save_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
        
    # 3. å‡†å¤‡é…ç½®æ•°æ®
    # ğŸŒŸ è‡ªåŠ¨æå– Display Metric
    auto_metric = _extract_metric_name(metric_config, default="Accuracy")
    
    config_data = {
        "meta_id": meta.id,
        "config_name": f"{name}_{mode}",
        "mode": mode,
        "file_path": abs_path,
        "reader_cfg": reader_cfg,
        "infer_cfg": infer_cfg,
        "metric_config": metric_config,
        "display_metric": auto_metric
    }

    # 4. ğŸŒŸ æ‰§è¡Œ Pydantic æ ¡éªŒ
    # å¦‚æœ reader_cfg ç¼ºå°‘å­—æ®µï¼Œæˆ– JSON æ ¼å¼é”™è¯¯ï¼Œè¿™é‡Œä¼šç›´æ¥æŠ›å‡º 422 é”™è¯¯
    try:
        validated_config = DatasetConfigCreate(**config_data)
    except ValueError as e:
        # åˆ é™¤å·²ä¸Šä¼ çš„åƒåœ¾æ–‡ä»¶
        if os.path.exists(save_path):
            os.remove(save_path)
        raise HTTPException(status_code=400, detail=f"é…ç½®æ ¡éªŒå¤±è´¥: {str(e)}")

    # 5. å­˜å…¥æ•°æ®åº“
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒååŒæ¨¡å¼çš„é…ç½®
    existing_config = next((c for c in meta.configs if c.mode == mode), None)
    
    if existing_config:
        # æ›´æ–°ç°æœ‰é…ç½®
        existing_config.file_path = validated_config.file_path
        existing_config.reader_cfg = validated_config.reader_cfg
        existing_config.infer_cfg = validated_config.infer_cfg
        existing_config.metric_config = validated_config.metric_config
        existing_config.display_metric = validated_config.display_metric
        session.add(existing_config)
    else:
        # åˆ›å»ºæ–°é…ç½®
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ validated_config.dict() æ¥ç¡®ä¿ä½¿ç”¨çš„æ˜¯æ¸…æ´—åçš„æ•°æ®
        # ä½† exclude è¯¥ model ä¸åŒ…å«çš„å­—æ®µ (å¦‚ meta_id å·²ç»åœ¨ db model é‡Œå®šä¹‰äº†)
        db_config = DatasetConfig(**validated_config.model_dump())
        session.add(db_config)
    
    session.commit()
    session.refresh(meta)
    return meta

@router.get("/", response_model=List[DatasetMetaRead])
def read_datasets(session: Session = Depends(get_session)):
    # è¿™é‡Œçš„ DatasetMetaRead åŒ…å« configs åˆ—è¡¨
    # ç¡®ä¿ unique() ä»¥é¿å… join äº§ç”Ÿçš„é‡å¤è¡Œ
    datasets = session.exec(select(DatasetMeta)).unique().all()
    return datasets

@router.delete("/{meta_id}")
def delete_dataset(meta_id: int, session: Session = Depends(get_session)):
    meta = session.get(DatasetMeta, meta_id)
    if not meta:
        raise HTTPException(status_code=404, detail="Dataset not found")
    
    # çº§è”åˆ é™¤æ–‡ä»¶
    for config in meta.configs:
        if os.path.exists(config.file_path):
            try:
                os.remove(config.file_path)
            except:
                pass
        
    session.delete(meta)
    session.commit()
    return {"ok": True}