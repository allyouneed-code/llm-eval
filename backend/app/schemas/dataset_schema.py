import json
from typing import Optional, List
from datetime import datetime
from sqlmodel import SQLModel
from pydantic import field_validator, model_validator

# ==========================================
# Level 3: è¯„ä¼°æŒ‡æ ‡ (Metric)
# ==========================================
class EvaluationMetricBase(SQLModel):
    evaluator_type: str
    name: str
    eval_cfg: str = "{}"

class EvaluationMetricRead(EvaluationMetricBase):
    id: int

# ==========================================
# Level 2: è¯„æµ‹é…ç½® (Config)
# ==========================================
class DatasetConfigBase(SQLModel):
    config_name: str
    
    # ğŸ†• æ–°å¢ï¼šä»»åŠ¡ç±»å‹ (è§„èŒƒçº¦å®šï¼šqa / multiple_choice / cloze)
    # ç”¨äºå‰ç«¯å›æ˜¾å’Œä¸šåŠ¡é€»è¾‘åˆ†æµ
    task_type: str = "qa" 
    
    mode: str = "gen" # gen / ppl
    prompt_version: Optional[str] = None
    
    # ğŸŒŸ UIå±•ç¤ºç”¨çš„æŒ‡æ ‡åç§° (å¦‚ "Accuracy", "BLEU")
    display_metric: str = "Accuracy"
    
    # é…ç½®è¯¦æƒ… (JSON å­—ç¬¦ä¸²)
    reader_cfg: str = "{}"
    infer_cfg: str = "{}"
    metric_config: str = "{}" # å¯¹åº” evaluator_config

    # ğŸ†• æ–°å¢å­—æ®µ (Baseä¸­å®šä¹‰ï¼ŒCreate/Readè‡ªåŠ¨ç»§æ‰¿)
    post_process_cfg: str = "{}"  # ç­”æ¡ˆæå–é…ç½®
    few_shot_cfg: str = "{}"      # å°‘æ ·æœ¬é…ç½®

# ğŸŒŸ æ–°å¢ï¼šç”¨äºåˆ›å»ºé…ç½®çš„ Schemaï¼ŒåŒ…å«æ ¡éªŒé€»è¾‘
class DatasetConfigCreate(DatasetConfigBase):
    meta_id: int
    file_path: str  # å¿…é¡»æŒ‡å®šæ–‡ä»¶è·¯å¾„

    # --- æ ¡éªŒ 1: ç¡®ä¿æ‰€æœ‰ cfg å­—æ®µéƒ½æ˜¯åˆæ³•çš„ JSON ---
    @field_validator('reader_cfg', 'infer_cfg', 'metric_config', 'post_process_cfg', 'few_shot_cfg')
    def must_be_valid_json(cls, v):
        try:
            parsed = json.loads(v)
            if not isinstance(parsed, dict):
                raise ValueError("Content must be a JSON object (dict)")
            return v
        except json.JSONDecodeError:
            raise ValueError("Invalid JSON string format")

    # --- æ ¡éªŒ 2: Reader å¿…é¡»åŒ…å«è§„èŒƒçº¦å®šçš„å­—æ®µ ---
    @field_validator('reader_cfg')
    def validate_reader(cls, v):
        try:
            cfg = json.loads(v)
            
            # 1. æ£€æŸ¥ OpenCompass è¿è¡Œå¿…éœ€å­—æ®µ
            if 'input_columns' not in cfg or 'output_column' not in cfg:
                raise ValueError("Reader Config must contain 'input_columns' and 'output_column'")
            
            # ğŸ†• 2. æ£€æŸ¥å‰ç«¯å›æ˜¾å¿…éœ€å­—æ®µ (è§„èŒƒçº¦å®š)
            # å¼ºåˆ¶è¦æ±‚å¿…é¡»å­˜ mappingï¼Œå¦åˆ™æ‹’ç»åˆ›å»º
            if 'mapping' not in cfg:
                raise ValueError("Reader Config must contain 'mapping' for frontend display")
            
            if not isinstance(cfg['mapping'], dict):
                raise ValueError("'mapping' field must be a dictionary")
                
        except json.JSONDecodeError:
            pass # æ ¼å¼é”™è¯¯ä¼šåœ¨ validate_json ä¸­è¢«æ•è·ï¼Œè¿™é‡Œå¿½ç•¥
        except Exception as e:
            # æŠ›å‡ºå…·ä½“ä¸šåŠ¡é”™è¯¯
            raise ValueError(f"Reader Config convention error: {str(e)}")
        return v

    # --- æ ¡éªŒ 3: PPL æ¨¡å¼ä¸‹çš„ç‰¹æ®Šé€»è¾‘ ---
    @model_validator(mode='after')
    def validate_mode_logic(self):
        # ... (ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜) ...
        if self.mode == 'ppl':
            try:
                infer_data = json.loads(self.infer_cfg)
                prompt_cfg = infer_data.get('prompt_template', {})
                template = prompt_cfg.get('template')
                
                if template and not isinstance(template, dict):
                    raise ValueError("In PPL mode, prompt_template.template must be a dictionary mapping options to prompts")
            except Exception as e:
                if "dictionary" in str(e):
                    raise e
        return self

class DatasetConfigRead(DatasetConfigBase):
    id: int
    created_at: datetime = datetime.utcnow()
    file_path: str
    
    metrics: List[EvaluationMetricRead] = []

# ==========================================
# Level 1: æ•°æ®é›†å…ƒæ•°æ® (Meta)
# ==========================================
class DatasetMetaBase(SQLModel):
    name: str
    category: str = "Base"
    description: Optional[str] = None
    modality: str = "Text"
    
    # ğŸ†• ä¿æŒä¹‹å‰æ·»åŠ çš„è½¯åˆ é™¤å­—æ®µå®šä¹‰ï¼ˆå¦‚æœä¹‹å‰åœ¨ Model åŠ äº†ï¼ŒSchema æœ€å¥½ä¹Ÿä½“ç°ï¼Œæˆ–è€…åœ¨ Read ä¸­ä½“ç°ï¼‰
    # ä½†é€šå¸¸ Base é‡Œä¸æ”¾ is_deleted é¿å…åˆ›å»ºæ—¶è¢«ç¯¡æ”¹ï¼Œè¿™é‡Œåªéœ€ Read é‡Œæœ‰å³å¯
    # is_deleted: bool = False 

class DatasetMetaCreate(DatasetMetaBase):
    pass

class DatasetMetaRead(DatasetMetaBase):
    id: int
    created_at: datetime
    # ğŸ†• è½¯åˆ é™¤æ ‡è®°
    is_deleted: bool 
    
    # ğŸŒŸ å…³é”®ï¼šåœ¨åˆ—è¡¨é¡µç›´æ¥è¿”å› configs
    configs: List[DatasetConfigRead] = []

class DatasetMetaDetail(DatasetMetaRead):
    pass

class DatasetPaginationResponse(SQLModel):
    total: int
    items: List[DatasetMetaRead]

# === ğŸŒŸ æ–°å¢ï¼šåˆ†ç±»ç»Ÿè®¡ç»“æ„ ===
class CategoryStat(SQLModel):
    category: str
    count: int