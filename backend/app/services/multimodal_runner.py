import os
import time
import json
import csv
import random
import logging
from typing import List, Dict, Any
from datetime import datetime

from app.models.dataset import DatasetConfig
from app.models.llm_model import LLMModel

logger = logging.getLogger(__name__)

class MultimodalRunner:
    """
    å¤šæ¨¡æ€è¯„æµ‹è¿è¡Œå™¨ (Simulation Version)
    ç›®å‰ç”¨äºæ¨¡æ‹Ÿ VLMEvalKit çš„è¡Œä¸ºï¼š
    1. æ£€æŸ¥å›¾ç‰‡/è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    2. æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹ (sleep)
    3. ç”Ÿæˆä¼ªé€ çš„ CSV ç»“æœ
    """

    def __init__(self, workspace: str):
        self.workspace = workspace
        os.makedirs(self.workspace, exist_ok=True)
        # æ¨¡æ‹Ÿ OpenCompass çš„ç»“æœç›®å½•ç»“æ„: {workspace}/{timestamp}/summary/
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = os.path.join(self.workspace, self.timestamp, "summary")
        os.makedirs(self.output_dir, exist_ok=True)

    def run(self, task_id: int, model: LLMModel, datasets: List[DatasetConfig]):
        """
        æ‰§è¡Œå¤šæ¨¡æ€è¯„æµ‹ (Mock)
        """
        logger.info(f"ğŸš€ [MultimodalRunner] Starting simulation for Task {task_id}...")
        
        # 1. æ¨¡æ‹Ÿç¯å¢ƒæ£€æŸ¥ä¸èµ„æºæ ¡éªŒ
        log_path = os.path.join(self.workspace, "multimodal_run.log")
        with open(log_path, "w", encoding="utf-8") as log_f:
            log_f.write(f"=== Multimodal Eval Simulation Start: {datetime.now()} ===\n")
            log_f.write(f"Model: {model.name} (Type: {model.type})\n")
            
            for ds in datasets:
                log_f.write(f"\nChecking dataset: {ds.config_name} ({ds.meta.modality})\n")
                
                # æ£€æŸ¥ JSONL æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(ds.file_path):
                    msg = f"âŒ Index file not found: {ds.file_path}"
                    log_f.write(msg + "\n")
                    logger.error(msg)
                    # çœŸå®åœºæ™¯å¯èƒ½ä¼šæŠ›å¼‚å¸¸ï¼Œæ¨¡æ‹Ÿåœºæ™¯æˆ‘ä»¬è®°å½•é”™è¯¯ä½†ç»§ç»­
                    continue

                # æ¨¡æ‹Ÿï¼šéšæœºæŠ½æŸ¥å‡ ä¸ªèµ„æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                # (è¿™é‡Œç®€å•è¯»å– jsonl çš„å‰å‡ è¡Œæ¥æ£€æŸ¥)
                try:
                    with open(ds.file_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()[:5] # åªæŸ¥å‰5è¡Œ
                        for line in lines:
                            item = json.loads(line)
                            # æ ¹æ®æ¨¡æ€æ‰¾å­—æ®µ
                            media_path = None
                            if ds.meta.modality == 'Image':
                                media_path = item.get('image')
                            elif ds.meta.modality == 'Video':
                                media_path = item.get('video')
                            elif ds.meta.modality == 'Audio':
                                media_path = item.get('audio')
                            
                            if media_path:
                                # æ‹¼æ¥ç»å¯¹è·¯å¾„ (å‡è®¾ jsonl åŒçº§ç›®å½•ä¸‹)
                                base_dir = os.path.dirname(ds.file_path)
                                abs_media = os.path.join(base_dir, media_path)
                                if os.path.exists(abs_media):
                                    log_f.write(f"  âœ… Asset found: {media_path}\n")
                                else:
                                    log_f.write(f"  âš ï¸ Asset MISSING: {media_path}\n")
                except Exception as e:
                    log_f.write(f"  âŒ Error reading index: {e}\n")

            # 2. æ¨¡æ‹Ÿæ¨ç†è€—æ—¶
            # æ ¹æ®æ•°æ®é›†æ•°é‡ï¼Œæ¯ä¸ª sleep 2ç§’ï¼Œå‡è£…åœ¨è·‘ GPU
            log_f.write("\nRunning inference on GPU (Simulated)...\n")
            time.sleep(2 * len(datasets)) 
            log_f.write("Inference finished.\n")

        # 3. ç”Ÿæˆç»“æœ CSV
        # æ ¼å¼å¿…é¡»ä¸ OpenCompassRunner.parse_results å…¼å®¹
        # å¿…éœ€åˆ—: dataset, version, metric, mode, {model_name}
        
        csv_filename = f"summary_{self.timestamp}.csv"
        csv_path = os.path.join(self.output_dir, csv_filename)
        
        model_col_name = model.name  # æˆ–è€…æ˜¯ model.abbr
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            # å†™å…¥è¡¨å¤´
            header = ['dataset', 'version', 'metric', 'mode', model_col_name]
            writer.writerow(header)
            
            for ds in datasets:
                # ç”Ÿæˆä¸€ä¸ª 60~95 ä¹‹é—´çš„éšæœºåˆ†
                fake_score = round(random.uniform(60.0, 95.0), 2)
                
                # ä¸ºäº†çœ‹èµ·æ¥çœŸå®ç‚¹ï¼ŒImage ä»»åŠ¡å¯èƒ½åˆ†ä½ä¸€ç‚¹
                if ds.meta.modality == 'Image':
                    fake_score = round(random.uniform(50.0, 85.0), 2)
                
                row = [
                    ds.config_name,       # dataset
                    '-',                  # version
                    ds.display_metric,    # metric (e.g. Accuracy)
                    'gen',                # mode
                    fake_score            # score
                ]
                writer.writerow(row)
        
        logger.info(f"âœ… [MultimodalRunner] Simulation finished. Result: {csv_path}")
        return csv_path