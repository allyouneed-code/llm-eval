import json
import os
import shutil
from datetime import datetime
from fastapi import HTTPException
from sqlmodel import Session, select
from typing import List, Optional, Dict, Any

from app.models.task import EvaluationTask
from app.models.llm_model import LLMModel
from app.models.dataset import DatasetConfig
from app.models.links import TaskDatasetLink
from app.models.result import EvaluationResult
from app.models.scheme import EvaluationScheme
from app.schemas.task_schema import TaskCreate
# ğŸ†• å¼•å…¥ Runner
from app.services.opencompass_runner import OpenCompassRunner

class TaskService:
    def __init__(self, session: Session):
        self.session = session

    # create_task, delete_task, get_task ç­‰æ–¹æ³•ä¿æŒä¸å˜...
    
    def create_task(self, task_in: TaskCreate) -> EvaluationTask:
        model = self.session.get(LLMModel, task_in.model_id)
        if not model:
            raise HTTPException(status_code=404, detail="æ‰€é€‰æ¨¡å‹ä¸å­˜åœ¨")
        
        target_config_ids = task_in.config_ids or []

        if task_in.scheme_id:
            scheme = self.session.get(EvaluationScheme, task_in.scheme_id)
            if not scheme:
                raise HTTPException(status_code=404, detail="æ‰€é€‰è¯„æµ‹æ–¹æ¡ˆä¸å­˜åœ¨")
            scheme_configs = scheme.configs
            if not scheme_configs:
                raise HTTPException(status_code=400, detail="è¯¥è¯„æµ‹æ–¹æ¡ˆæœªåŒ…å«ä»»ä½•æœ‰æ•ˆçš„æ•°æ®é›†é…ç½®")
            target_config_ids = [c.id for c in scheme_configs]
        
        if not target_config_ids:
            raise HTTPException(status_code=400, detail="æœªé€‰æ‹©ä»»ä½•è¯„æµ‹æ•°æ®é›†")

        statement = select(DatasetConfig).where(DatasetConfig.id.in_(target_config_ids))
        configs = self.session.exec(statement).all()
        
        if len(configs) != len(set(target_config_ids)):
             raise HTTPException(status_code=400, detail="éƒ¨åˆ†è¯„æµ‹é…ç½®ä¸å­˜åœ¨æˆ–IDé‡å¤")
        
        datasets_json = json.dumps([c.id for c in configs])
        
        db_task = EvaluationTask(
            model_id=task_in.model_id,
            datasets_list=datasets_json,
            scheme_id=task_in.scheme_id,
            status="pending",
            progress=0
        )
        self.session.add(db_task)
        self.session.commit()
        self.session.refresh(db_task)
        
        for config in configs:
            snapshot_json = json.dumps(config.model_dump(mode='json'), default=str)
            link = TaskDatasetLink(
                task_id=db_task.id,
                dataset_config_id=config.id,
                config_snapshot=snapshot_json
            )
            self.session.add(link)
        
        self.session.commit()
        return db_task

    def delete_task(self, task_id: int) -> bool:
        task = self.get_task(task_id)
        if not task:
            return False
        results = self.session.exec(select(EvaluationResult).where(EvaluationResult.task_id == task_id)).all()
        for r in results:
            self.session.delete(r)
        links = self.session.exec(select(TaskDatasetLink).where(TaskDatasetLink.task_id == task_id)).all()
        for l in links:
            self.session.delete(l)  
        self.session.delete(task)
        self.session.commit()
        return True

    def get_task(self, task_id: int) -> Optional[EvaluationTask]:
        return self.session.get(EvaluationTask, task_id)

    def get_all_tasks(self) -> List[EvaluationTask]:
        return self.session.exec(select(EvaluationTask)).all()

    # ====================================================
    # ğŸŒŸ æ ¸å¿ƒé€»è¾‘
    # ====================================================
    def run_evaluation_logic(self, task_id: int):
        """
        æ‰§è¡Œè¯„æµ‹ä»»åŠ¡ (Real Implementation)
        """
        # 1. è·å–ä»»åŠ¡ä¸ä¸Šä¸‹æ–‡
        task = self.get_task(task_id)
        if not task:
            return "Task Not Found"

        # æ›´æ–°çŠ¶æ€ä¸º Running
        task.status = "running"
        task.progress = 1
        task.error_msg = None
        self.session.add(task)
        self.session.commit()

        try:
            # 2. å‡†å¤‡æ•°æ®å¯¹è±¡
            model = self.session.get(LLMModel, task.model_id)
            if not model:
                raise ValueError(f"Model {task.model_id} not found")

            # è§£ææ•°æ®é›†é…ç½®
            config_ids = []
            try:
                config_ids = json.loads(task.datasets_list)
            except:
                pass
            
            configs = self.session.exec(
                select(DatasetConfig).where(DatasetConfig.id.in_(config_ids))
            ).all()

            if not configs:
                raise ValueError("No datasets found for this task")

            # 3. åˆå§‹åŒ– Runner
            # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„å·¥ä½œç›®å½•ï¼Œé¿å…å†²çª
            task_workspace = os.path.join(os.getcwd(), "workspace", "tasks", f"task_{task_id}")
            runner = OpenCompassRunner(workspace=task_workspace)
            
            # æ›´æ–°è¿›åº¦
            task.progress = 5
            self.session.add(task)
            self.session.commit()

            # 4. ç”Ÿæˆé…ç½®æ–‡ä»¶
            print(f"ğŸ“„ [Task {task_id}] Generating config...")
            config_path = runner.generate_config(task_id, model, configs)
            
            task.progress = 10
            self.session.add(task)
            self.session.commit()

            # 5. æ‰§è¡Œè¯„æµ‹
            print(f"ğŸš€ [Task {task_id}] Running OpenCompass...")
            runner.run(config_path)
            
            # è¿è¡Œå®Œæˆåï¼Œè¿›åº¦è·³åˆ° 90%
            task.progress = 90
            self.session.add(task)
            self.session.commit()

            # 6. è§£æç»“æœå¹¶å…¥åº“
            print(f"ğŸ“Š [Task {task_id}] Parsing results...")
            raw_results = runner.parse_results()
            
            table_data = [] # ç”¨äºå‰ç«¯å±•ç¤ºçš„æ‘˜è¦è¡¨
            
            for res in raw_results:
                # å¯»æ‰¾å¯¹åº”çš„ config å¯¹è±¡
                matched_config = None
                dataset_abbr = res['dataset']
                
                for cfg in configs:
                    # æ¨¡ç³ŠåŒ¹é… config name
                    if cfg.meta.name in dataset_abbr or dataset_abbr in cfg.meta.name:
                        matched_config = cfg
                        break
                
                target_config_id = matched_config.id if matched_config else configs[0].id
                dataset_name_display = matched_config.meta.name if matched_config else dataset_abbr
                dataset_category = matched_config.meta.category if matched_config else "Unknown"
                
                # å†™å…¥æ•°æ®åº“ EvaluationResult
                db_result = EvaluationResult(
                    task_id=task_id,
                    dataset_config_id=target_config_id,
                    dataset_name=dataset_name_display,
                    metric_name=res['metric'],
                    score=res['score'],
                    details=res['raw_data']
                )
                self.session.add(db_result)
                
                # æ”¶é›†å‰ç«¯å±•ç¤ºæ•°æ®
                table_data.append({
                    "dataset": dataset_name_display,
                    "capability": dataset_category,
                    "metric": res['metric'],
                    "score": res['score']
                })

            # 7. ç”Ÿæˆæœ€ç»ˆçš„ä»»åŠ¡æ‘˜è¦ (Radar + Table)
            final_summary = self._generate_summary(table_data)
            
            task.result_summary = json.dumps(final_summary)
            task.status = "success"
            task.progress = 100
            task.finished_at = datetime.now()
            
            print(f"âœ… [Task {task_id}] Finished successfully.")

        except Exception as e:
            import traceback
            traceback.print_exc()
            task.status = "failed"
            task.error_msg = str(e)
            print(f"âŒ [Task {task_id}] Failed: {e}")
        
        finally:
            self.session.add(task)
            self.session.commit()
            return f"Task {task_id} processed"

    def _generate_summary(self, table_data: List[Dict]) -> Dict:
        """
        æ ¹æ®ç»“æœç”Ÿæˆé›·è¾¾å›¾å’Œè¡¨æ ¼æ•°æ®
        ã€æ”¹è¿›ã€‘ç»Ÿä¸€é‡çº² (0-1 -> 0-100) å¹¶å¤„ç†ç‰¹æ®ŠæŒ‡æ ‡
        """
        if not table_data:
            return {"radar": [], "table": []}

        capability_stats = {}
        
        for item in table_data:
            cat = item['capability']
            try:
                raw_score = float(item['score'])
            except (ValueError, TypeError):
                raw_score = 0.0
                
            metric = str(item['metric']).lower()
            
            # --- å½’ä¸€åŒ–é€»è¾‘ ---
            norm_score = raw_score
            
            # 1. è¯†åˆ«è´Ÿå‘æŒ‡æ ‡ (è¶Šå°è¶Šå¥½)
            # ä¾‹å¦‚: ppl (Perplexity), loss, bpb (Bits Per Byte)
            # è¿™ç±»æŒ‡æ ‡ä¸é€‚åˆç›´æ¥å‚ä¸ 0-100 çš„é›·è¾¾å›¾å¹³å‡ï¼Œæš‚æ—¶æ‰“æ ‡è·³è¿‡æˆ–ä¿ç•™åŸå€¼
            # (å¦‚æœåŒä¸€èƒ½åŠ›ç»´åº¦ä¸‹æ··åˆäº† Acc å’Œ PPLï¼Œç›´æ¥å¹³å‡ä¼šå¾ˆå¥‡æ€ªï¼Œæ‰€ä»¥è¿™é‡Œç­–ç•¥æ˜¯å°½é‡åªå–æ­£å‘æŒ‡æ ‡å¹³å‡)
            is_negative_metric = any(x in metric for x in ['ppl', 'bpb', 'loss'])
            
            if is_negative_metric:
                # è´Ÿå‘æŒ‡æ ‡æš‚æ—¶ä¸å‚ä¸é›·è¾¾å›¾çš„â€œèƒ½åŠ›å¾—åˆ†â€è®¡ç®—
                # é™¤éä½ å¸Œæœ›æ˜¾ç¤ºä¸€ä¸ªå¾ˆä½çš„åˆ†æ•°ï¼ˆå› ä¸º PPL è¶Šä½è¶Šå¥½ï¼Œä½†åœ¨é›·è¾¾å›¾ä¸Šä½åˆ†ä¼šè¢«è®¤ä¸ºæ˜¯å¼±ï¼‰
                continue 
            else:
                # 2. è¯†åˆ« 0-1 åˆ†æ•° (Acc, BLEU, ROUGE ç­‰)
                # å¯å‘å¼è§„åˆ™ï¼šå¦‚æœåˆ†æ•°åœ¨ 0.0 åˆ° 1.0 ä¹‹é—´ï¼Œå¤§æ¦‚ç‡æ˜¯å°æ•°åˆ¶ï¼Œè½¬æ¢ä¸ºç™¾åˆ†åˆ¶
                if 0.0 <= raw_score <= 1.0:
                    norm_score = raw_score * 100.0
            
            if cat not in capability_stats:
                capability_stats[cat] = []
            
            capability_stats[cat].append(norm_score)
        
        radar_data = []
        for cat, scores in capability_stats.items():
            if not scores:
                # å¦‚æœè¯¥ç»´åº¦ä¸‹åªæœ‰ PPL æŒ‡æ ‡ï¼Œå¯èƒ½ scores ä¸ºç©º
                # è¿™ç§æƒ…å†µä¸‹ï¼Œç»™ä¸€ä¸ªé»˜è®¤æ˜¾ç¤ºï¼ˆæ¯”å¦‚ 0 æˆ–è€…ä¸æ˜¾ç¤ºï¼‰
                continue

            # è®¡ç®—å¹³å‡åˆ†
            avg_score = sum(scores) / len(scores)
            
            # é™åˆ¶æœ€å¤§æ˜¾ç¤ºä¸º 100 (é˜²æ­¢éƒ¨åˆ†å¼‚å¸¸æŒ‡æ ‡æº¢å‡º)
            display_score = min(avg_score, 100.0)
            
            radar_data.append({
                "name": cat,
                "max": 100,
                "score": round(display_score, 1)
            })
            
        return {
            "radar": radar_data,
            "table": table_data
        }