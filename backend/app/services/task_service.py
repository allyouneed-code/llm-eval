import json
import os
import shutil
import time
from datetime import datetime
from fastapi import HTTPException
from sqlmodel import Session, select
from typing import List, Optional, Dict, Any

from app.models.task import EvaluationTask
from app.models.llm_model import LLMModel
from app.models.dataset import DatasetConfig
from app.models.links import TaskDatasetLink
from app.models.result import EvaluationResult
from app.models.scheme import EvaluationScheme
from app.schemas.task_schema import TaskCreate
# ğŸ†• å¼•å…¥ Runner
from app.services.opencompass_runner import OpenCompassRunner

class TaskService:
    def __init__(self, session: Session):
        self.session = session

    # create_task, delete_task, get_task ç­‰æ–¹æ³•ä¿æŒä¸å˜...
    
    def create_task(self, task_in: TaskCreate) -> EvaluationTask:
        model = self.session.get(LLMModel, task_in.model_id)
        if not model:
            raise HTTPException(status_code=404, detail="æ‰€é€‰æ¨¡å‹ä¸å­˜åœ¨")
        
        target_config_ids = task_in.config_ids or []

        if task_in.scheme_id:
            scheme = self.session.get(EvaluationScheme, task_in.scheme_id)
            if not scheme:
                raise HTTPException(status_code=404, detail="æ‰€é€‰è¯„æµ‹æ–¹æ¡ˆä¸å­˜åœ¨")
            scheme_configs = scheme.configs
            if not scheme_configs:
                raise HTTPException(status_code=400, detail="è¯¥è¯„æµ‹æ–¹æ¡ˆæœªåŒ…å«ä»»ä½•æœ‰æ•ˆçš„æ•°æ®é›†é…ç½®")
            target_config_ids = [c.id for c in scheme_configs]
        
        if not target_config_ids:
            raise HTTPException(status_code=400, detail="æœªé€‰æ‹©ä»»ä½•è¯„æµ‹æ•°æ®é›†")

        statement = select(DatasetConfig).where(DatasetConfig.id.in_(target_config_ids))
        configs = self.session.exec(statement).all()
        
        if len(configs) != len(set(target_config_ids)):
             raise HTTPException(status_code=400, detail="éƒ¨åˆ†è¯„æµ‹é…ç½®ä¸å­˜åœ¨æˆ–IDé‡å¤")
        
        datasets_json = json.dumps([c.id for c in configs])
        
        db_task = EvaluationTask(
            model_id=task_in.model_id,
            datasets_list=datasets_json,
            scheme_id=task_in.scheme_id,
            status="pending",
            progress=0
        )
        self.session.add(db_task)
        self.session.commit()
        self.session.refresh(db_task)
        
        for config in configs:
            snapshot_json = json.dumps(config.model_dump(mode='json'), default=str)
            link = TaskDatasetLink(
                task_id=db_task.id,
                dataset_config_id=config.id,
                config_snapshot=snapshot_json
            )
            self.session.add(link)
        
        self.session.commit()
        return db_task

    def delete_task(self, task_id: int) -> bool:
        task = self.get_task(task_id)
        if not task:
            return False
        results = self.session.exec(select(EvaluationResult).where(EvaluationResult.task_id == task_id)).all()
        for r in results:
            self.session.delete(r)
        links = self.session.exec(select(TaskDatasetLink).where(TaskDatasetLink.task_id == task_id)).all()
        for l in links:
            self.session.delete(l)  
        self.session.delete(task)
        self.session.commit()
        return True

    def get_task(self, task_id: int) -> Optional[EvaluationTask]:
        return self.session.get(EvaluationTask, task_id)

    def get_all_tasks(self) -> List[EvaluationTask]:
        return self.session.exec(select(EvaluationTask)).all()

    # ====================================================
    # ğŸŒŸ æ ¸å¿ƒé€»è¾‘
    # ====================================================
    def run_evaluation_logic(self, task_id: int):
        """
        æ‰§è¡Œè¯„æµ‹ä»»åŠ¡ (Real Implementation)
        """
        # 1. è·å–ä»»åŠ¡ä¸ä¸Šä¸‹æ–‡
        task = self.get_task(task_id)
        if not task:
            return "Task Not Found"

        # æ›´æ–°çŠ¶æ€ä¸º Running
        task.status = "running"
        task.progress = 1
        task.error_msg = None
        self.session.add(task)
        self.session.commit()

        try:
            # 2. å‡†å¤‡æ•°æ®å¯¹è±¡
            model = self.session.get(LLMModel, task.model_id)
            if not model:
                raise ValueError(f"Model {task.model_id} not found")

            # è§£ææ•°æ®é›†é…ç½®
            config_ids = []
            try:
                config_ids = json.loads(task.datasets_list)
            except:
                pass
            
            configs = self.session.exec(
                select(DatasetConfig).where(DatasetConfig.id.in_(config_ids))
            ).all()

            if not configs:
                raise ValueError("No datasets found for this task")

            # 3. åˆå§‹åŒ– Runner
            # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„å·¥ä½œç›®å½•ï¼Œé¿å…å†²çª
            task_workspace = os.path.join(os.getcwd(), "workspace", "tasks", f"task_{task_id}")
            runner = OpenCompassRunner(workspace=task_workspace)
            
            # æ›´æ–°è¿›åº¦
            task.progress = 5
            self.session.add(task)
            self.session.commit()

            # 4. ç”Ÿæˆé…ç½®æ–‡ä»¶
            print(f"ğŸ“„ [Task {task_id}] Generating config...")
            config_path = runner.generate_config(task_id, model, configs)
            
            task.progress = 10
            self.session.add(task)
            self.session.commit()

            # 5. æ‰§è¡Œè¯„æµ‹
            print(f"ğŸš€ [Task {task_id}] Running OpenCompass...")
            start_time = time.time()
            runner.run(config_path)
            end_time = time.time()
            total_duration = end_time - start_time

            # è¿è¡Œå®Œæˆåï¼Œè¿›åº¦è·³åˆ° 90%
            task.progress = 90
            self.session.add(task)
            self.session.commit()

            # 6. è§£æç»“æœå¹¶å…¥åº“
            print(f"ğŸ“Š [Task {task_id}] Parsing results...")
            raw_results = runner.parse_results()
            
            table_data = [] # ç”¨äºå‰ç«¯å±•ç¤ºçš„æ‘˜è¦è¡¨
            
            for res in raw_results:
                # å¯»æ‰¾å¯¹åº”çš„ config å¯¹è±¡
                matched_config = None
                dataset_abbr = res['dataset']
                
                for cfg in configs:
                    # æ¨¡ç³ŠåŒ¹é… config name
                    if cfg.meta.name in dataset_abbr or dataset_abbr in cfg.meta.name:
                        matched_config = cfg
                        break
                
                target_config_id = matched_config.id if matched_config else configs[0].id
                dataset_name_display = matched_config.meta.name if matched_config else dataset_abbr
                dataset_category = matched_config.meta.category if matched_config else "Unknown"
                
                # å†™å…¥æ•°æ®åº“ EvaluationResult
                db_result = EvaluationResult(
                    task_id=task_id,
                    dataset_config_id=target_config_id,
                    dataset_name=dataset_name_display,
                    metric_name=res['metric'],
                    score=res['score'],
                    details=res['raw_data']
                )
                self.session.add(db_result)
                
                # æ”¶é›†å‰ç«¯å±•ç¤ºæ•°æ®
                table_data.append({
                    "dataset": dataset_name_display,
                    "capability": dataset_category,
                    "metric": res['metric'],
                    "score": res['score']
                })

            # 7. ç”Ÿæˆæœ€ç»ˆçš„ä»»åŠ¡æ‘˜è¦ (Radar + Table)
            final_summary = self._generate_summary(table_data)
            final_summary["time_stats"] = {
                "total_duration": round(total_duration, 2),
                "avg_per_dataset": round(total_duration / len(configs), 2) if configs else 0
            }
            
            task.result_summary = json.dumps(final_summary)
            task.status = "success"
            task.progress = 100
            task.finished_at = datetime.now()
            
            print(f"âœ… [Task {task_id}] Finished successfully.")

        except Exception as e:
            import traceback
            traceback.print_exc()
            task.status = "failed"
            task.error_msg = str(e)
            print(f"âŒ [Task {task_id}] Failed: {e}")
        
        finally:
            self.session.add(task)
            self.session.commit()
            return f"Task {task_id} processed"

    def _generate_summary(self, table_data: List[Dict]) -> Dict:
        """
        æ ¹æ®ç»“æœç”Ÿæˆé›·è¾¾å›¾å’Œè¡¨æ ¼æ•°æ®
        ã€æ”¹è¿›ã€‘ç»Ÿä¸€é‡çº² (0-1 -> 0-100) å¹¶å¤„ç†ç‰¹æ®ŠæŒ‡æ ‡
        """
        if not table_data:
            return {"radar": [], "table": []}

        capability_stats = {}
        
        for item in table_data:
            cat = item['capability']
            try:
                raw_score = float(item['score'])
            except (ValueError, TypeError):
                raw_score = 0.0
                
            metric = str(item['metric']).lower()
            
            # --- å½’ä¸€åŒ–é€»è¾‘ ---
            norm_score = raw_score
            
            # 1. è¯†åˆ«è´Ÿå‘æŒ‡æ ‡ (è¶Šå°è¶Šå¥½)
            # ä¾‹å¦‚: ppl (Perplexity), loss, bpb (Bits Per Byte)
            # è¿™ç±»æŒ‡æ ‡ä¸é€‚åˆç›´æ¥å‚ä¸ 0-100 çš„é›·è¾¾å›¾å¹³å‡ï¼Œæš‚æ—¶æ‰“æ ‡è·³è¿‡æˆ–ä¿ç•™åŸå€¼
            # (å¦‚æœåŒä¸€èƒ½åŠ›ç»´åº¦ä¸‹æ··åˆäº† Acc å’Œ PPLï¼Œç›´æ¥å¹³å‡ä¼šå¾ˆå¥‡æ€ªï¼Œæ‰€ä»¥è¿™é‡Œç­–ç•¥æ˜¯å°½é‡åªå–æ­£å‘æŒ‡æ ‡å¹³å‡)
            is_negative_metric = any(x in metric for x in ['ppl', 'bpb', 'loss'])
            
            if is_negative_metric:
                # è´Ÿå‘æŒ‡æ ‡æš‚æ—¶ä¸å‚ä¸é›·è¾¾å›¾çš„â€œèƒ½åŠ›å¾—åˆ†â€è®¡ç®—
                # é™¤éä½ å¸Œæœ›æ˜¾ç¤ºä¸€ä¸ªå¾ˆä½çš„åˆ†æ•°ï¼ˆå› ä¸º PPL è¶Šä½è¶Šå¥½ï¼Œä½†åœ¨é›·è¾¾å›¾ä¸Šä½åˆ†ä¼šè¢«è®¤ä¸ºæ˜¯å¼±ï¼‰
                continue 
            else:
                # 2. è¯†åˆ« 0-1 åˆ†æ•° (Acc, BLEU, ROUGE ç­‰)
                # å¯å‘å¼è§„åˆ™ï¼šå¦‚æœåˆ†æ•°åœ¨ 0.0 åˆ° 1.0 ä¹‹é—´ï¼Œå¤§æ¦‚ç‡æ˜¯å°æ•°åˆ¶ï¼Œè½¬æ¢ä¸ºç™¾åˆ†åˆ¶
                if 0.0 <= raw_score <= 1.0:
                    norm_score = raw_score * 100.0
            
            if cat not in capability_stats:
                capability_stats[cat] = []
            
            capability_stats[cat].append(norm_score)
        
        radar_data = []
        for cat, scores in capability_stats.items():
            if not scores:
                # å¦‚æœè¯¥ç»´åº¦ä¸‹åªæœ‰ PPL æŒ‡æ ‡ï¼Œå¯èƒ½ scores ä¸ºç©º
                # è¿™ç§æƒ…å†µä¸‹ï¼Œç»™ä¸€ä¸ªé»˜è®¤æ˜¾ç¤ºï¼ˆæ¯”å¦‚ 0 æˆ–è€…ä¸æ˜¾ç¤ºï¼‰
                continue

            # è®¡ç®—å¹³å‡åˆ†
            avg_score = sum(scores) / len(scores)
            
            # é™åˆ¶æœ€å¤§æ˜¾ç¤ºä¸º 100 (é˜²æ­¢éƒ¨åˆ†å¼‚å¸¸æŒ‡æ ‡æº¢å‡º)
            display_score = min(avg_score, 100.0)
            
            radar_data.append({
                "name": cat,
                "max": 100,
                "score": round(display_score, 1)
            })
            
        return {
            "radar": radar_data,
            "table": table_data
        }
    def compare_tasks(self, task_ids: List[int]) -> Dict[str, Any]:
        """
        å¯¹æ¯”å¤šä¸ªä»»åŠ¡çš„ç»“æœ (å¿…é¡»åŸºäºåŒä¸€ Scheme)
        """
        if len(task_ids) < 2:
            raise HTTPException(status_code=400, detail="è‡³å°‘é€‰æ‹©ä¸¤ä¸ªä»»åŠ¡è¿›è¡Œå¯¹æ¯”")
            
        # 1. æ‰¹é‡æŸ¥è¯¢ä»»åŠ¡
        tasks = self.session.exec(
            select(EvaluationTask).where(EvaluationTask.id.in_(task_ids))
        ).all()
        
        if len(tasks) != len(task_ids):
             raise HTTPException(status_code=404, detail="éƒ¨åˆ†ä»»åŠ¡æœªæ‰¾åˆ°")

        # 2. æ ¡éªŒ Scheme ä¸€è‡´æ€§
        base_scheme_id = tasks[0].scheme_id
        if not base_scheme_id:
             raise HTTPException(status_code=400, detail="æ— æ³•å¯¹æ¯”æœªç»‘å®šæ–¹æ¡ˆçš„ä»»åŠ¡")
             
        for t in tasks:
            if t.scheme_id != base_scheme_id:
                raise HTTPException(status_code=400, detail="æ‰€æœ‰ä»»åŠ¡å¿…é¡»å±äºåŒä¸€ä¸ªè¯„æµ‹æ–¹æ¡ˆ")
        
        scheme = self.session.get(EvaluationScheme, base_scheme_id)
        scheme_name = scheme.name if scheme else "Unknown Scheme"

        # 3. å‡†å¤‡æ¨¡å‹å…ƒæ•°æ® (Model Metadata)
        # ç»“æ„: [{id: 1, name: "Llama2", task_id: 101}, ...]
        models_meta = []
        task_id_to_model_name = {}
        
        for t in tasks:
            model = self.session.get(LLMModel, t.model_id)
            model_name = model.name if model else f"Unknown-{t.model_id}"
            # å¦‚æœåŒä¸€ä¸ªæ¨¡å‹è·‘äº†å¤šæ¬¡ï¼ŒåŠ ä¸Š Task ID åŒºåˆ†
            display_name = f"{model_name} (#{t.id})"
            
            models_meta.append({
                "task_id": t.id,
                "model_name": model_name,
                "display_name": display_name,
                "finished_at": t.finished_at
            })
            task_id_to_model_name[t.id] = display_name

        # 4. æ‹‰å–ç»“æœå¹¶ç»“æ„åŒ–
        # æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªä»¥ {dataset_name + metric} ä¸º key çš„å­—å…¸
        # table_map = { "GSM8K (Accuracy)": { 101: 85.5, 102: 88.0 }, ... }
        table_map = {}
        
        # åŒæ—¶æ”¶é›†èƒ½åŠ›ç»´åº¦ç”¨äºé›·è¾¾å›¾
        # capability_map = { 101: {"Reasoning": [80, 90], "Coding": [20]}, 102: ... }
        capability_map = {t.id: {} for t in tasks}

        # æ‰¹é‡æ‹‰å–æ‰€æœ‰ç»“æœ
        all_results = self.session.exec(
            select(EvaluationResult).where(EvaluationResult.task_id.in_(task_ids))
        ).all()
        
        for res in all_results:
            # --- 4.1 å‡†å¤‡è¡¨æ ¼æ•°æ® ---
            row_key = f"{res.dataset_name} ({res.metric_name})"
            if row_key not in table_map:
                table_map[row_key] = {
                    "dataset": res.dataset_name,
                    "metric": res.metric_name,
                    "scores": {} # { task_id: score }
                }
            table_map[row_key]["scores"][res.task_id] = res.score
            
            # --- 4.2 å‡†å¤‡é›·è¾¾å›¾æ•°æ® (éœ€è¦æŸ¥ Dataset Config æ‹¿ capability) ---
            # è¿™é‡Œçš„æ€§èƒ½ä¼˜åŒ–ç‚¹ï¼šå¯ä»¥é¢„å…ˆæ‰¹é‡æŸ¥å¥½ DatasetConfigï¼Œé¿å…å¾ªç¯æŸ¥åº“
            # æ—¢ç„¶æ˜¯åŒ Schemeï¼Œæ•°æ®é›†é…ç½®æ˜¯ä¸€æ ·çš„ï¼Œè¿™é‡Œç®€å•å¤„ç†ï¼Œå‡è®¾ dataset_name èƒ½å¯¹åº”ä¸Š
            # æ›´å¥½çš„åšæ³•æ˜¯åœ¨ EvaluationResult é‡Œå†—ä½™å­˜å‚¨ capabilityï¼Œæˆ–è€…åœ¨è¿™é‡Œå¤šæŸ¥ä¸€æ¬¡
            # æš‚æ—¶ç”¨ä¸€ç§ç®€å•çš„æ–¹å¼ï¼šå›æŸ¥ DatasetConfig (å®é™…é¡¹ç›®ä¸­å»ºè®®åŠ ç¼“å­˜)
            config = self.session.get(DatasetConfig, res.dataset_config_id)
            if config:
                cat = config.meta.category
                if cat not in capability_map[res.task_id]:
                    capability_map[res.task_id][cat] = []
                
                # ç®€å•çš„å½’ä¸€åŒ–é€»è¾‘ (åŒ _generate_summary)
                score_val = res.score
                if 0 <= score_val <= 1.0: score_val *= 100
                capability_map[res.task_id][cat].append(score_val)

        # 5. æ„å»ºæœ€ç»ˆè¾“å‡ºæ ¼å¼
        
        # 5.1 ç»„è£… Table List
        final_table = []
        for key, info in table_map.items():
            row = {
                "dataset_metric": key,
                "dataset": info['dataset'],
                "metric": info['metric']
            }
            # å¡«å…¥æ¯ä¸ªä»»åŠ¡çš„åˆ†æ•°
            base_score = None # ç”¨äºè®¡ç®— diff (ä»¥ç¬¬ä¸€ä¸ªä»»åŠ¡ä¸ºåŸºå‡†)
            
            for i, task_id in enumerate(task_ids):
                score = info["scores"].get(task_id, None)
                row[f"task_{task_id}"] = score
                
                # è®¡ç®—ä¸ç¬¬ä¸€ä¸ªä»»åŠ¡çš„ Diff
                if i == 0:
                    base_score = score
                else:
                    if base_score is not None and score is not None:
                        row[f"diff_{task_id}"] = round(score - base_score, 2)
                    else:
                        row[f"diff_{task_id}"] = None
            
            final_table.append(row)

        # 5.2 ç»„è£… Radar Data
        radar_indicators = [] # ECharts indicator { name: 'Math', max: 100 }
        radar_series = []     # ECharts series data
        
        # æ”¶é›†æ‰€æœ‰å‡ºç°è¿‡çš„èƒ½åŠ›ç»´åº¦
        all_categories = set()
        for t_map in capability_map.values():
            all_categories.update(t_map.keys())
        
        sorted_cats = sorted(list(all_categories))
        radar_indicators = [{"name": c, "max": 100} for c in sorted_cats]
        
        for task_id in task_ids:
            task_scores = []
            for cat in sorted_cats:
                scores_list = capability_map[task_id].get(cat, [])
                if scores_list:
                    avg = sum(scores_list) / len(scores_list)
                    task_scores.append(round(min(avg, 100), 1))
                else:
                    task_scores.append(0) # ç¼ºå¤±ç»´åº¦è¡¥0
            
            radar_series.append({
                "name": task_id_to_model_name[task_id],
                "value": task_scores
            })

        return {
            "scheme_name": scheme_name,
            "models": models_meta,
            "radar_indicators": radar_indicators,
            "radar_data": radar_series,
            "table_data": final_table
        }