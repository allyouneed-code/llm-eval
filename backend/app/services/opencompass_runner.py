import os
import subprocess
import glob
import logging
import torch
import pandas as pd
from typing import List, Dict, Any
from app.models.llm_model import LLMModel
from app.models.dataset import DatasetConfig

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class OpenCompassRunner:
    def __init__(self, workspace: str):
        """
        åˆå§‹åŒ–è¿è¡Œå™¨
        :param workspace: ä»»åŠ¡çš„å·¥ä½œç›®å½•ï¼Œç”¨äºå­˜æ”¾ config.py, æ—¥å¿—å’Œè¾“å‡ºç»“æœ
        """
        self.workspace = workspace
        # ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
        os.makedirs(self.workspace, exist_ok=True)

    def _detect_device_config(self) -> Dict[str, Any]:
        """
        ã€ç¯å¢ƒæ¢æµ‹ã€‘
        æ£€æµ‹å½“å‰è¿è¡Œç¯å¢ƒï¼ˆGPU/CPUï¼‰ï¼Œè¿”å›é€‚é…çš„è¿è¡Œå‚æ•°
        """
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            logger.info(f"ğŸš€ Detected {gpu_count} GPUs. Using GPU mode.")
            return {
                "device_map": "'auto'",
                "num_gpus": 1,          # å•ä¸ªä»»åŠ¡é»˜è®¤å ç”¨ 1 å¼ å¡
                "max_out_len": 100,
                "batch_size": 8,        # æ˜¾å­˜è¶³å¤Ÿæ—¶è°ƒå¤§ï¼ŒåŠ å¿«é€Ÿåº¦
            }
        else:
            logger.warning("âš ï¸ No GPU detected. Falling back to CPU mode (Very Slow).")
            return {
                "device_map": "'cpu'",
                "num_gpus": 0,
                "max_out_len": 20,      # CPU æ¨¡å¼ä¸‹ç¼©çŸ­è¾“å‡ºé•¿åº¦
                "batch_size": 1,
            }

    def generate_config(self, task_id: int, model: LLMModel, datasets: List[DatasetConfig]) -> str:
        """
        ã€é…ç½®ç”Ÿæˆã€‘
        åŸºäºâ€œå¼•ç”¨â€æ¨¡å¼ç”Ÿæˆé…ç½®æ–‡ä»¶ã€‚
        ä¸é‡æ–°å®šä¹‰æ•°æ®é›†ï¼Œè€Œæ˜¯ç›´æ¥å¼•ç”¨æ•°æ®åº“ä¸­å­˜å‚¨çš„ dataset.path
        """
        run_cfg = self._detect_device_config()
        
        # 1. å‡†å¤‡æ•°æ®é›†è·¯å¾„åˆ—è¡¨
        # è¿™é‡Œçš„ ds.path åº”è¯¥æ˜¯ OpenCompass å®¹å™¨å†…çš„æœ‰æ•ˆè·¯å¾„
        # ä¾‹å¦‚: 'configs/datasets/gsm8k/gsm8k_gen.py' (å®˜æ–¹)
        # æˆ–è€…: 'workspace/custom_configs/my_data_gen.py' (ç§æœ‰)
        dataset_paths_list = [f"'{ds.path}'" for ds in datasets]
        base_datasets_str = ",\n    ".join(dataset_paths_list)

        # 2. æ‹¼æ¥å®Œæ•´çš„ Python é…ç½®å­—ç¬¦ä¸²
        # æ ¸å¿ƒé€»è¾‘ï¼š
        # (1) ä½¿ç”¨ _base_ åŠ è½½æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶
        # (2) éå† locals() æ‰¾åˆ°æ‰€æœ‰åŠ è½½è¿›æ¥çš„æ•°æ®é›†å˜é‡ (é€šå¸¸ä»¥ _datasets ç»“å°¾)
        # (3) å°†å®ƒä»¬åˆå¹¶åˆ°æœ€ç»ˆçš„ datasets åˆ—è¡¨ä¸­
        config_content = f"""
from opencompass.models import HuggingFaceCausalLM

# 1. å¼•ç”¨å¤–éƒ¨æ•°æ®é›†é…ç½®
_base_ = [
    {base_datasets_str}
]

# 2. è‡ªåŠ¨èšåˆæ•°æ®é›†
# OpenCompass çš„æ•°æ®é›†é…ç½®æ–‡ä»¶é€šå¸¸ä¼šå®šä¹‰ä¸€ä¸ªå˜é‡ï¼Œå¦‚ gsm8k_datasets
# è¿™é‡Œæˆ‘ä»¬éœ€è¦æŠŠè¿™äº›åˆ†æ•£çš„å˜é‡æ”¶é›†åˆ°ä¸€ä¸ªåä¸º datasets çš„æ€»åˆ—è¡¨ä¸­
datasets = []
for _k, _v in locals().items():
    if _k.endswith('_datasets') and isinstance(_v, list):
        datasets.extend(_v)

# 3. å®šä¹‰æ¨¡å‹
models = [
    dict(
        type=HuggingFaceCausalLM,
        abbr='{model.name}',
        path='{model.path}',           # æ¨¡å‹åœ¨å®¹å™¨å†…çš„ç»å¯¹è·¯å¾„
        tokenizer_path='{model.path}',
        model_kwargs=dict(
            device_map={run_cfg['device_map']},
            trust_remote_code=True
        ),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True
        ),
        max_out_len={run_cfg['max_out_len']},
        max_seq_len=2048,
        batch_size={run_cfg['batch_size']},
        run_cfg=dict(num_gpus={run_cfg['num_gpus']}),
    )
]

# 4. ç»“æœæ±‡æ€»é…ç½® (å¯é€‰ï¼Œè‡ªåŠ¨æ ¹æ®æ•°æ®é›†ç”Ÿæˆæ±‡æ€»è¡¨)
# ç®€å•çš„è‡ªåŠ¨æ±‡æ€»é…ç½®
summarizer = dict(
    dataset_abbrs=[ds['abbr'] for ds in datasets],
    summary_groups=sum([ds.get('summary_groups', []) for ds in datasets], []),
)

# 5. æŒ‡å®šå·¥ä½œç›®å½•
work_dir = '{self.workspace}'
"""
        
        # 3. å†™å…¥æ–‡ä»¶
        config_path = os.path.join(self.workspace, f"task_{task_id}_config.py")
        with open(config_path, "w", encoding="utf-8") as f:
            f.write(config_content)
        
        logger.info(f"âœ… Generated config file at: {config_path}")
        return config_path

    def run(self, config_path: str, log_file_name: str = "output.log"):
        """
        ã€è¿›ç¨‹æ‰§è¡Œã€‘
        è°ƒç”¨å­è¿›ç¨‹æ‰§è¡Œ OpenCompass å‘½ä»¤
        """
        log_path = os.path.join(self.workspace, log_file_name)
        
        # æ„é€ å‘½ä»¤: opencompass config.py -w work_dir --debug
        cmd = [
            "opencompass", 
            config_path, 
            "-w", self.workspace,
            "--debug"  # ä¿æŒ debug ä»¥ä¾¿æ’é”™
        ]

        logger.info(f"â–¶ï¸ Starting OpenCompass execution: {' '.join(cmd)}")

        with open(log_path, "w", encoding="utf-8") as f_log:
            # ä½¿ç”¨ Popen è°ƒç”¨
            process = subprocess.Popen(
                cmd,
                stdout=f_log,
                stderr=subprocess.STDOUT,  # å°† stderr åˆå¹¶åˆ° stdout
                text=True,
                bufsize=1  # è¡Œç¼“å†²ï¼Œä¿è¯æ—¥å¿—å®æ—¶å†™å…¥
            )
            
            # é˜»å¡ç­‰å¾…ç»“æŸ
            return_code = process.wait()
            
            if return_code != 0:
                logger.error(f"âŒ OpenCompass failed with exit code {return_code}. Check logs at {log_path}")
                raise RuntimeError(f"OpenCompass execution failed. Log: {log_path}")
            
            logger.info("âœ… OpenCompass execution finished successfully.")

    def parse_results(self) -> List[Dict[str, Any]]:
        """
        ã€ç»“æœè§£æã€‘
        æŸ¥æ‰¾æœ€æ–°çš„ summary.csv å¹¶è§£æç»“æœ
        """
        # OpenCompass è¾“å‡ºç›®å½•ç»“æ„: workspace/{timestamp}/summary/summary.csv
        search_pattern = os.path.join(self.workspace, "*", "summary", "summary.csv")
        csv_files = glob.glob(search_pattern)
        
        if not csv_files:
            logger.warning("âš ï¸ No summary.csv found. Evaluation might have failed.")
            return []
        
        # å–æœ€æ–°çš„æ–‡ä»¶
        latest_csv = max(csv_files, key=os.path.getmtime)
        logger.info(f"ğŸ“Š Parsing results from: {latest_csv}")
        
        try:
            # è¯»å– CSV
            df = pd.read_csv(latest_csv)
            
            results = []
            # ç®€å•è§£æé€»è¾‘ï¼šå‡è®¾æˆ‘ä»¬è¦æŠŠæ¯ä¸€è¡Œéƒ½å­˜ä¸‹æ¥
            # é€šå¸¸ summary.csv çš„åˆ—åŒ…æ‹¬ dataset, version, metric, mode, <model_name>
            
            # ä¸ºäº†é€šç”¨æ€§ï¼Œæˆ‘ä»¬å°†ç»“æœè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼Œäº¤ç»™ TaskService å»å†³å®šå­˜å“ªäº›å­—æ®µ
            # ä¾‹å¦‚: [{'dataset': 'GSM8K', 'accuracy': 88.5}, ...]
            
            for _, row in df.iterrows():
                # è½¬æ¢æ•´è¡Œä¸ºå­—å…¸
                row_dict = row.to_dict()
                
                # åšä¸€ç‚¹ç®€å•çš„æ•°æ®æ¸…æ´—
                # æå–æ•°æ®é›†åç§°ï¼Œé€šå¸¸ç¬¬ä¸€åˆ—æ˜¯ dataset
                clean_result = {
                    "dataset": row_dict.get("dataset", "Unknown"),
                    "metric": row_dict.get("metric", "score"),
                    "mode": row_dict.get("mode", "unknown"),
                    "raw_data": row_dict  # ä¿ç•™åŸå§‹æ•°æ®ä»¥å¤‡ä¸æ—¶ä¹‹éœ€
                }
                
                # å°è¯•æŸ¥æ‰¾åˆ†æ•°ï¼šé€šå¸¸æ˜¯æœ€åä¸€åˆ—ï¼Œæˆ–è€…åˆ—åç­‰äºæ¨¡å‹åçš„é‚£ä¸€åˆ—
                # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„å¯å‘å¼æŸ¥æ‰¾ï¼šæ‰¾æœ€åä¸€ä¸ªæ˜¯æ•°å­—çš„åˆ—
                score = 0.0
                for col in reversed(df.columns):
                    val = row_dict[col]
                    if isinstance(val, (int, float)) and col not in ['version']:
                        score = val
                        break
                
                clean_result["score"] = score
                results.append(clean_result)
                
            return results

        except Exception as e:
            logger.error(f"âŒ Failed to parse CSV: {e}")
            raise e