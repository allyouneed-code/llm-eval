import os
import subprocess
import glob
import logging
import torch
import json
import pandas as pd
from typing import List, Dict, Any
from app.models.llm_model import LLMModel
from app.models.dataset import DatasetConfig

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class OpenCompassRunner:
    def __init__(self, workspace: str):
        """
        åˆå§‹åŒ–è¿è¡Œå™¨
        :param workspace: ä»»åŠ¡çš„ç‹¬ç«‹å·¥ä½œç›®å½•ï¼Œç”¨äºå­˜æ”¾ config.py, æ—¥å¿—å’Œè¾“å‡ºç»“æœ
        """
        self.workspace = workspace
        # ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
        os.makedirs(self.workspace, exist_ok=True)
        
        # å®šä¹‰å®˜æ–¹é…ç½®æ–‡ä»¶çš„æ ¹ç›®å½• (å‡è®¾è¿è¡Œåœ¨ backend ç›®å½•ä¸‹ï¼Œæ•°æ®å­˜æ”¾åœ¨ data/official)
        # å¦‚æœæ˜¯ Docker ç¯å¢ƒï¼Œé€šå¸¸æ˜¯ /app/data/official
        self.official_data_root = os.path.abspath(os.path.join("data", "official"))

    def _detect_device_config(self) -> Dict[str, Any]:
        """
        ã€ç¯å¢ƒæ¢æµ‹ã€‘
        æ£€æµ‹å½“å‰è¿è¡Œç¯å¢ƒï¼ˆGPU/CPUï¼‰ï¼Œè¿”å›é€‚é…çš„è¿è¡Œå‚æ•°
        """
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            logger.info(f"ğŸš€ Detected {gpu_count} GPUs. Using GPU mode.")
            return {
                "device_map": "'auto'",
                "num_gpus": 1,          # é»˜è®¤å•ä»»åŠ¡å•å¡ï¼Œå¯æ ¹æ®è°ƒåº¦ä¼˜åŒ–
                "max_out_len": 100,
                "batch_size": 8,        
            }
        else:
            logger.warning("âš ï¸ No GPU detected. Falling back to CPU mode (Very Slow).")
            return {
                "device_map": "'cpu'",
                "num_gpus": 0,
                "max_out_len": 20,      
                "batch_size": 1,
            }

    def generate_config(self, task_id: int, model: LLMModel, datasets: List[DatasetConfig]) -> str:
        """
        ã€é…ç½®ç”Ÿæˆã€‘
        ç”Ÿæˆç”¨äº OpenCompass è¿è¡Œçš„ Python é…ç½®æ–‡ä»¶
        æ”¯æŒæ··åˆåŠ è½½ï¼š
        1. ç§æœ‰æ•°æ®é›† (JSONL + åŠ¨æ€ç”Ÿæˆ Config)
        2. å®˜æ–¹æ•°æ®é›† (åŠ è½½æœ¬åœ° .py é…ç½®æ–‡ä»¶)
        """
        
        # 1. å‡†å¤‡è·¯å¾„
        workspace_str = str(os.path.abspath(self.workspace)).replace("\\", "/")
        
        # =========================================================
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šç”Ÿæˆ dataset_loader.py (ç”¨äºç§æœ‰æ•°æ®é›†)
        # =========================================================
        loader_code = [
            "import json",
            "import os",
            "from opencompass.datasets import BaseDataset",
            "from datasets import Dataset",
            "",
            "class SimpleJsonlDataset(BaseDataset):",
            "    def load(self, path):",
            "        data_list = []",
            "        if not os.path.exists(path):",
            "            print(f'Warning: Dataset file not found: {path}')",
            "            return {'test': Dataset.from_list([]), 'train': Dataset.from_list([])}",
            "        with open(path, 'r', encoding='utf-8') as f:",
            "            for line in f:",
            "                line = line.strip()",
            "                if line:",
            "                    try:",
            "                        data_list.append(json.loads(line))",
            "                    except:",
            "                        pass",
            "        dataset = Dataset.from_list(data_list)",
            "        return {",
            "            'test': dataset,",
            "            'train': dataset,",
            "            'validation': dataset",
            "        }"
        ]
        
        loader_path = os.path.join(self.workspace, "dataset_loader.py")
        with open(loader_path, "w", encoding="utf-8") as f:
            f.write("\n".join(loader_code))
            
        # =========================================================
        # ç¬¬äºŒéƒ¨åˆ†ï¼šå‡†å¤‡é…ç½®å˜é‡
        # =========================================================
        run_cfg = self._detect_device_config()
        
        # --- 2.1 æ„å»ºæ•°æ®é›†åˆ—è¡¨ (æ··åˆæ¨¡å¼) ---
        private_ds_lines = []
        official_read_lines = []
        official_ds_vars = []

        for idx, ds in enumerate(datasets):
            # ğŸŸ¢ åˆ†æ”¯ Aï¼šå®˜æ–¹æ•°æ®é›† (çº¦å®š file_path ä»¥ official:// å¼€å¤´)
            if ds.file_path and ds.file_path.startswith("official://"):
                # è§£æçœŸå®è·¯å¾„
                # æ•°æ®åº“å­˜: official://configs/gsm8k/gsm8k_gen.py
                # çœŸå®è·¯å¾„: /app/data/official/configs/gsm8k/gsm8k_gen.py
                relative_path = ds.file_path.replace("official://", "")
                real_config_path = os.path.join(self.official_data_root, relative_path)
                
                # è·¯å¾„è½¬ä¹‰ (Windowså…¼å®¹)
                real_config_path = str(real_config_path).replace("\\", "/")
                
                if not os.path.exists(real_config_path):
                    logger.error(f"âŒ Official config file missing: {real_config_path}")
                    # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œæš‚æ—¶è·³è¿‡ï¼Œé˜²æ­¢æ•´ä¸ªä»»åŠ¡æŒ‚æ‰
                    continue

                var_name = f"official_ds_{idx}"
                
                # ç”Ÿæˆè¯»å–å®˜æ–¹é…ç½®çš„ä»£ç 
                code_block = [
                    f"",
                    f"# --- Official Dataset: {ds.config_name} ---",
                    f"# Loading from: {real_config_path}",
                    f"_tmp_cfg_{idx} = Config.fromfile('{real_config_path}')",
                    f"# å°è¯•æå– datasets å˜é‡ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ª list",
                    f"{var_name} = _tmp_cfg_{idx}.get('datasets', [])",
                    f"# Force override 'abbr' to match DB config_name for result mapping",
                    f"for item in {var_name}:",
                    f"    item['abbr'] = '{ds.config_name}'"
                ]
                official_read_lines.extend(code_block)
                official_ds_vars.append(var_name)
                continue

            # ğŸ”µ åˆ†æ”¯ Bï¼šç§æœ‰æ•°æ®é›† (JSONL)
            # 1. è·¯å¾„å¤„ç†
            fpath = str(ds.file_path).replace("\\", "/") if ds.file_path else ""
            if fpath and not os.path.isabs(fpath):
                 fpath = os.path.abspath(fpath).replace("\\", "/")

            # 2. è§£æ JSON é…ç½®
            try: reader_cfg = json.loads(ds.reader_cfg) if ds.reader_cfg else {}
            except: reader_cfg = {}
            try: infer_cfg = json.loads(ds.infer_cfg) if ds.infer_cfg else {}
            except: infer_cfg = {}
            try: metric_cfg = json.loads(ds.metric_config) if getattr(ds, 'metric_config', None) else {}
            except: metric_cfg = {}
            try: post_process_cfg = json.loads(ds.post_process_cfg) if getattr(ds, 'post_process_cfg', None) else {}
            except: post_process_cfg = {}

            # 3. é€»è¾‘æ•´åˆ
            
            # (A) ç»„è£… eval_cfg
            eval_cfg = metric_cfg.copy()
            if not eval_cfg.get('evaluator'):
                eval_cfg['evaluator'] = {'type': 'AccEvaluator'}
            
            # æ³¨å…¥åå¤„ç†é…ç½®
            if post_process_cfg and post_process_cfg.get("type"):
                eval_cfg["pred_postprocessor"] = dict(
                    type=post_process_cfg["type"],
                    **{k: v for k, v in post_process_cfg.items() if k != "type"}
                )

            # (B) æ¸…ç† reader_cfg (ç§»é™¤å‰ç«¯ mapping)
            clean_reader_cfg = {k: v for k, v in reader_cfg.items() if k != 'mapping'}
            if not clean_reader_cfg:
                clean_reader_cfg = dict(input_columns=['question', 'textA', 'textB', 'textC', 'textD'],output_column='answerKey')

            # (C) å…œåº• infer_cfg
            if not infer_cfg:
                 infer_cfg = {
                    'prompt_template': {
                        'type': 'PromptTemplate',
                        'template': dict(round=[dict(role='HUMAN', prompt='Question: {question}\nAnswer:')])
                    },
                    'retriever': {'type': 'ZeroRetriever'},
                    'inferencer': {'type': 'GenInferencer'}
                 }

            item = {
                'abbr': ds.config_name,
                'type': 'SimpleJsonlDataset',
                'path': fpath,
                'reader_cfg': clean_reader_cfg,
                'infer_cfg': infer_cfg,
                'eval_cfg': eval_cfg
            }
            private_ds_lines.append(f"    dict({json.dumps(item, ensure_ascii=False)}),")
            
        # --- 2.2 åˆå¹¶æ‰€æœ‰æ•°æ®é›† ---
        # ç”Ÿæˆç§æœ‰ datasets åˆ—è¡¨ä»£ç 
        if private_ds_lines:
            private_block = "private_datasets = [\n" + "\n".join(private_ds_lines) + "\n]"
        else:
            private_block = "private_datasets = []"
            
        # ç”Ÿæˆåˆå¹¶ä»£ç : datasets = private_datasets + official_ds_0 + ...
        all_lists = ["private_datasets"] + official_ds_vars
        combine_block = f"datasets = {' + '.join(all_lists)}"

        # --- 2.3 æ„å»ºæ¨¡å‹åˆ—è¡¨ ---
        m_abbr = str(model.name)       # ä»…ç”¨äºæ—¥å¿—æ˜¾ç¤ºçš„ç®€ç§°
        m_model_id = str(model.path)   # API çœŸæ­£è°ƒç”¨çš„æ¨¡å‹ ID (å¦‚ gpt-4)
        m_key = str(model.api_key) if model.api_key else "" # é»˜è®¤é˜²ç©º
        m_base_url = str(model.base_url) if model.base_url else ""
        
        # å‡†å¤‡ Import è¯­å¥
        if model.type == "api":
            model_import_stmt = "from opencompass.models import OpenAI"
            models_block = f"""
models = [
    dict(
        type=OpenAI,
        abbr='{m_abbr}',              # è¯„æµ‹ç»“æœä¸­æ˜¾ç¤ºçš„åˆ—å
        path='{m_model_id}',          # ä¼ ç»™ API çš„æ¨¡å‹å‚æ•° (model="gpt-4")
        key='{m_key}',                # API Key
        openai_api_base='{m_base_url}', # API Base URL
        meta_template=dict(
            round=[
                dict(role='HUMAN', api_role='HUMAN'),
                dict(role='BOT', api_role='BOT', generate=True),
            ],
        ),
        query_per_second=1,
        max_out_len=2048,
        max_seq_len=4096,
        batch_size=1,
    )
]
"""
        else:
            model_import_stmt = "from opencompass.models import HuggingFaceCausalLM"
            m_local_path = str(model.path)
            models_block = f"""
models = [
    dict(
        type=HuggingFaceCausalLM,
        abbr='{m_abbr}',
        path='{m_local_path}',
        tokenizer_path='{m_local_path}',
        model_kwargs=dict(
            device_map={run_cfg['device_map']},
            trust_remote_code=True
        ),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True
        ),
        max_out_len={run_cfg['max_out_len']},
        max_seq_len=2048,
        batch_size={run_cfg['batch_size']},
        run_cfg=dict(num_gpus={run_cfg['num_gpus']}),
    )
]
"""

        # =========================================================
        # ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‹¼æ¥ä¸»é…ç½®æ–‡ä»¶
        # =========================================================
        main_config_lines = [
            "import sys",
            "import os",
            f"sys.path.append(r'{workspace_str}')", 
            # ğŸŒŸ å¿…é¡»å¯¼å…¥ Config ä»¥æ”¯æŒ fromfile åŠ è½½
            "from mmengine.config import Config",
            
            # å¯¼å…¥ OpenCompass é€šç”¨ç»„ä»¶
            "from opencompass.openicl.icl_prompt_template import PromptTemplate",
            "from opencompass.openicl.icl_retriever import ZeroRetriever",
            "from opencompass.openicl.icl_inferencer import GenInferencer",
            "from opencompass.openicl.icl_evaluator import AccEvaluator",
            "from opencompass.openicl.icl_evaluator import BleuEvaluator",
            "from opencompass.openicl.icl_evaluator import RougeEvaluator",
            "from opencompass.utils.text_postprocessors import first_option_postprocess",
            "from opencompass.utils.text_postprocessors import first_capital_postprocess",
            
            # å¯¼å…¥ç§æœ‰æ•°æ®é›†åŠ è½½å™¨
            "from dataset_loader import SimpleJsonlDataset",
            
            # å¯¼å…¥æ¨¡å‹ç±»
            model_import_stmt, 
            "",
            # ğŸŸ¢ 1. æ’å…¥å®˜æ–¹æ•°æ®é›†åŠ è½½ä»£ç 
            "\n".join(official_read_lines),
            "",
            # ğŸ”µ 2. æ’å…¥ç§æœ‰æ•°æ®é›†å®šä¹‰
            private_block,
            "",
            # ğŸŸ¡ 3. åˆå¹¶åˆ—è¡¨
            combine_block,
            "",
            # 4. ç±»å‹ä¿®æ­£ (Type Fixes for Private Datasets)
            # å› ä¸ºç§æœ‰æ•°æ®é›†æ˜¯é€šè¿‡ JSON æ‹¼è£…çš„ï¼Œç±»åæ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦æ›¿æ¢ä¸ºçœŸå®ç±»å¼•ç”¨
            "for ds in datasets:",
            "    # åªå¤„ç†ç§æœ‰æ•°æ®é›† (SimpleJsonlDataset)",
            "    if ds.get('type') == 'SimpleJsonlDataset':",
            "        ds['type'] = SimpleJsonlDataset",
            "",
            "        # Infer Config Types",
            "        if 'infer_cfg' in ds:",
            "            if ds['infer_cfg'].get('prompt_template', {}).get('type') == 'PromptTemplate':",
            "                ds['infer_cfg']['prompt_template']['type'] = PromptTemplate",
            "            if ds['infer_cfg'].get('retriever', {}).get('type') == 'ZeroRetriever':",
            "                ds['infer_cfg']['retriever']['type'] = ZeroRetriever",
            "            if ds['infer_cfg'].get('inferencer', {}).get('type') == 'GenInferencer':",
            "                ds['infer_cfg']['inferencer']['type'] = GenInferencer",
            "",
            "        # Eval Config Types",
            "        if 'eval_cfg' in ds:",
            "            ev_type = ds['eval_cfg'].get('evaluator', {}).get('type')",
            "            if ev_type == 'AccEvaluator':",
            "                ds['eval_cfg']['evaluator']['type'] = AccEvaluator",
            "            elif ev_type == 'BleuEvaluator':",
            "                ds['eval_cfg']['evaluator']['type'] = BleuEvaluator",
            "            elif ev_type == 'RougeEvaluator':",
            "                ds['eval_cfg']['evaluator']['type'] = RougeEvaluator",
            "",
            "            pp_type = ds['eval_cfg'].get('pred_postprocessor', {}).get('type')",
            "            if pp_type == 'first_option_postprocess':",
            "                ds['eval_cfg']['pred_postprocessor']['type'] = first_option_postprocess",
            "            elif pp_type == 'first_capital_postprocess':",
            "                ds['eval_cfg']['pred_postprocessor']['type'] = first_capital_postprocess",
            "",
            models_block,
            "",
            "summarizer = dict(",
            "    dataset_abbrs=[ds['abbr'] for ds in datasets],",
            "    summary_groups=[],",
            ")",
            "",
            f"work_dir = r'{workspace_str}'",
            "",
            # æ¸…ç†å‘½åç©ºé—´ï¼Œé˜²æ­¢ç”Ÿæˆçš„ Config åŒ…å«ä¸å¿…è¦çš„å˜é‡
            "try:",
            "    del os, sys, SimpleJsonlDataset, OpenAI",
            "    del PromptTemplate, ZeroRetriever, GenInferencer, AccEvaluator",
            "except:",
            "    pass"
        ]
        
        config_path = os.path.join(self.workspace, f"task_{task_id}_config.py")
        with open(config_path, "w", encoding="utf-8") as f:
            f.write("\n".join(main_config_lines))
        
        logger.info(f"âœ… Generated config file: {config_path}")
        return config_path

    def run(self, config_path: str, log_file_name: str = "output.log"):
        """
        ã€è¿›ç¨‹æ‰§è¡Œã€‘
        """
        log_path = os.path.join(self.workspace, log_file_name)
        
        # æ„é€ å‘½ä»¤
        cmd = ["opencompass", config_path, "-w", self.workspace, "--debug"]
        logger.info(f"â–¶ï¸ Starting OpenCompass execution: {' '.join(cmd)}")

        with open(log_path, "w", encoding="utf-8") as f_log:
            process = subprocess.Popen(
                cmd,
                stdout=f_log,
                stderr=subprocess.STDOUT,
                text=True
            )
            return_code = process.wait()
            
            if return_code != 0:
                logger.error(f"âŒ OpenCompass execution failed. Log: {log_path}")
                raise RuntimeError(f"OpenCompass exited with code {return_code}")
            
            logger.info("âœ… OpenCompass execution finished successfully.")

    def parse_results(self) -> List[Dict[str, Any]]:
        """
        ã€ç»“æœè§£æã€‘
        è§£æç”Ÿæˆçš„ summary.csv
        """
        search_pattern = os.path.join(self.workspace, "*", "summary", "summary.csv")
        csv_files = glob.glob(search_pattern)
        
        if not csv_files:
            logger.warning("âš ï¸ No summary.csv found.")
            return []
        
        # å–æœ€æ–°ç”Ÿæˆçš„ CSV
        latest_csv = max(csv_files, key=os.path.getmtime)
        try:
            df = pd.read_csv(latest_csv)
            results = []
            
            for _, row in df.iterrows():
                row_dict = row.to_dict()
                
                # ç®€å•æ¸…æ´—
                dataset_abbr = row_dict.get("dataset", "Unknown")
                metric = row_dict.get("metric", "score")
                
                # æå–åˆ†æ•°ï¼šå–æœ€åä¸€åˆ—æ•°å€¼åˆ—ä½œä¸ºåˆ†æ•°
                score = 0.0
                for col in reversed(df.columns):
                    val = row_dict[col]
                    if isinstance(val, (int, float)) and col not in ['version', 'metric', 'mode']:
                        score = float(val)
                        break
                
                results.append({
                    "dataset": dataset_abbr,
                    "metric": metric,
                    "score": score,
                    "raw_data": row_dict
                })
                
            return results
        except Exception as e:
            logger.error(f"âŒ Failed to parse CSV: {e}")
            raise e