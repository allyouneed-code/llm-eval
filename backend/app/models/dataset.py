from typing import List, Optional, TYPE_CHECKING
from sqlmodel import SQLModel, Field, Relationship
from datetime import datetime
from sqlalchemy import Column, Text
# å¼•å…¥ Link è¡¨
from app.models.links import TaskDatasetLink
from app.models.scheme import SchemeDatasetLink, EvaluationScheme

if TYPE_CHECKING:
    from app.models.task import EvaluationTask
    from app.models.result import EvaluationResult

# ==========================================
# 1. æ•°æ®é›†å…ƒæ•°æ®è¡¨ (DatasetMeta)
# ==========================================
class DatasetMeta(SQLModel, table=True):
    __tablename__ = "dataset_metas"

    id: Optional[int] = Field(default=None, primary_key=True)
    
    name: str = Field(index=True, unique=True)
    category: str = Field(default="Base")
    description: Optional[str] = None
    
    # ğŸ†• æ–°å¢ï¼šè½¯åˆ é™¤æ ‡è®°
    is_deleted: bool = Field(default=False)

    modality: str = Field(default="Text") #æ•°æ®æ¨¡æ€ (Text, Image, Audio, Video)
    
    data_count: int = Field(default=0)
    # å…³ç³»å®šä¹‰ä¿æŒåŸæ ·ï¼Œä¸éœ€è¦åŠ  cascade="all, delete-orphan" äº†
    configs: List["DatasetConfig"] = Relationship(back_populates="meta")
    created_at: datetime = Field(default_factory=datetime.utcnow)


# ==========================================
# 2. è¯„æµ‹é…ç½®å˜ä½“è¡¨ (DatasetConfig)
# ==========================================
class DatasetConfig(SQLModel, table=True):
    __tablename__ = "dataset_configs"

    id: Optional[int] = Field(default=None, primary_key=True)
    
    # å¤–é”®
    meta_id: int = Field(foreign_key="dataset_metas.id")
    meta: Optional[DatasetMeta] = Relationship(back_populates="configs")

    # æ ‡è¯†
    config_name: str = Field(index=True)
    
    # ğŸŒŸ å…³é”®å­—æ®µï¼šç¡®ä¿è¿™äº›éƒ½åœ¨ï¼
    file_path: str 
    task_type: str = Field(default="multiple_choice", index=True)
    mode: str = Field(default="gen")         # gen / ppl
    prompt_version: Optional[str] = None
    display_metric: str = Field(default="Accuracy") 
    
    # å¤æ‚é…ç½® (å­˜ JSON)
    reader_cfg: str = Field(default="{}", sa_column=Column(Text)) 
    infer_cfg: str = Field(default="{}", sa_column=Column(Text))
    metric_config: str = Field(default="{}", sa_column=Column(Text)) 
    
    # åå¤„ç†é…ç½®
    post_process_cfg: str = Field(default="{}", sa_column=Column(Text)) 
    
    # å°‘æ ·æœ¬é…ç½®
    few_shot_cfg: str = Field(default="{}", sa_column=Column(Text))
    
    # =========================
    # ğŸ†• æ–°å¢å­—æ®µ End
    # =========================
    
    metrics: List["EvaluationMetric"] = Relationship(back_populates="config")
    
    created_at: datetime = Field(default_factory=datetime.utcnow)

    # ğŸŒŸ å…³ç³»å®šä¹‰
    tasks: List["EvaluationTask"] = Relationship(back_populates="datasets", link_model=TaskDatasetLink)
    results: List["EvaluationResult"] = Relationship(back_populates="dataset_config")

    schemes: List[EvaluationScheme] = Relationship(
        back_populates="configs", 
        link_model=SchemeDatasetLink
    )

# ==========================================
# 3. è¯„ä¼°æŒ‡æ ‡è¡¨ (EvaluationMetric)
# ==========================================
class EvaluationMetric(SQLModel, table=True):
    __tablename__ = "evaluation_metrics"

    id: Optional[int] = Field(default=None, primary_key=True)
    
    config_id: int = Field(foreign_key="dataset_configs.id")
    config: Optional[DatasetConfig] = Relationship(back_populates="metrics")
    
    evaluator_type: str
    name: str
    eval_cfg: str = Field(default="{}")