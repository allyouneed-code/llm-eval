{
  "ifeval": "Instruction Following",
  "nphardeval": "Reasoning",
  "pmmeval": "Language",
  "pi-llm": "Memory",
  "theroremqa": "Reasoning",
  "agieval": "Examination",
  "babilong": "Long Context",
  "bigcodebench": "Code",
  "calm": "Reasoning",
  "infinitebench (âbench)": "Long Context",
  "kor-bench": "Reasoning",
  "lawbench": "Knowledge",
  "l-eval": "Long Context",
  "livecodebench": "Code",
  "livemathbench": "Math",
  "livereasonbench": "Reasoning",
  "longbench": "Long Context",
  "lv-eval": "Long Context",
  "mastermath2024v1": "Math",
  "matbench": "Science",
  "medbench": "Knowledge",
  "medcalc_bench": "Knowledge",
  "medqa": "Knowledge",
  "medxpertqa": "Knowledge",
  "clinicbench": "Knowledge",
  "scienceqa": "Knowledge",
  "pubmedqa": "Knowledge",
  "musr": "Reasoning",
  "needlebench v1 (deprecated)": "Long Context",
  "needlebench v2": "Long Context",
  "ruler": "Long Context",
  "alignbench": "Subjective",
  "alpacaeval": "Subjective",
  "arena-hard": "Subjective",
  "flames": "Subjective",
  "fofo": "Subjective",
  "followbench": "Subjective",
  "hellobench": "Subjective",
  "judgerbench": "Subjective",
  "mt-bench-101": "Subjective",
  "wildbench": "Subjective",
  "t-eval": "Tool Utilization",
  "financeiq": "Knowledge",
  "gaokaobench": "Examination",
  "lcbench": "Code",
  "arabicmmlu": "Language",
  "openfindata": "Knowledge",
  "quality": "Long Context",
  "adversarial glue": "Safety",
  "clue / afqmc": "Language",
  "aime2024": "Examination",
  "adversarial nli": "Reasoning",
  "anthropics evals": "Safety",
  "apps": "Code",
  "arc": "Reasoning",
  "arc prize": "ARC-AGI",
  "superglue / ax": "Reasoning",
  "big-bench hard": "Reasoning",
  "big-bench extra hard": "Reasoning",
  "superglue / boolq": "Knowledge",
  "clue / c3 (câ³)": "Understanding",
  "cardbiomedbench": "Knowledge",
  "superglue / cb": "Reasoning",
  "c-eval": "Examination",
  "charm": "Reasoning",
  "chembench": "Knowledge",
  "fewclue / chid": "Language",
  "chinese simpleqa": "Knowledge",
  "cibench": "Code",
  "civilcomments": "Safety",
  "cloze test-max/min": "Code",
  "fewclue / cluewsc": "Language",
  "cmb": "Knowledge",
  "cmmlu": "Understanding",
  "clue / cmnli": "Reasoning",
  "cmo_fib": "Examination",
  "clue / cmrc": "Understanding",
  "commonsenseqa": "Knowledge",
  "commonsenseqa-cn": "Knowledge",
  "superglue / copa": "Reasoning",
  "crowspairs": "Safety",
  "crowspairs-cn": "Safety",
  "cvalues": "Safety",
  "clue / drcd": "Understanding",
  "drop (drop simple eval)": "Understanding",
  "ds-1000": "Code",
  "fewclue / eprstmt": "Understanding",
  "flores": "Language",
  "game24": "Math",
  "government report dataset": "Long Context",
  "gpqa": "Knowledge",
  "gsm8k": "Math",
  "gsm-hard": "Math",
  "hle(humanityâs last exam)": "Reasoning",
  "hellaswag": "Reasoning",
  "humaneval": "Code",
  "humaneval-cn": "Code",
  "multi-humaneval": "Code",
  "humaneval+": "Code",
  "humaneval-x": "Code",
  "humaneval pro": "Code",
  "hungarian_math": "Math",
  "iwslt2017": "Language",
  "jigsawmultilingual": "Safety",
  "lambada": "Understanding",
  "lcsts": "Understanding",
  "livestembench": "",
  "llm compression": "Bits Per Character (BPC)",
  "math": "Math",
  "math500": "Math",
  "math 401": "Math",
  "mathbench": "Math",
  "mbpp": "Code",
  "mbpp-cn": "Code",
  "mbpp-plus": "Code",
  "mbpp pro": "Code",
  "mgsm": "Language",
  "mmlu": "Understanding",
  "scieval": "Understanding",
  "mmlu-cf": "Understanding",
  "mmlu-pro": "Understanding",
  "mmmlu": "Language",
  "superglue / multirc": "Understanding",
  "multipl-e": "Code",
  "narrativeqa": "Understanding",
  "naturalquestions": "Knowledge",
  "naturalquestions-cn": "Knowledge",
  "openbookqa": "Knowledge",
  "olymmath": "Math",
  "proteinlmbench": "Knowledge",
  "py150": "Code",
  "qasper": "Long Context",
  "qasper-cut": "Long Context",
  "race": "Examination",
  "r-bench": "Reasoning",
  "realtoxicprompts": "Safety",
  "superglue / record": "Understanding",
  "superglue / rte": "Reasoning",
  "clue / ocnli": "Reasoning",
  "fewclue / ocnli-fc": "Reasoning",
  "rolebench": "Role Play",
  "s3eval": "Long Context",
  "scibench": "Reasoning",
  "scicode": "Code",
  "seedbench": "Knowledge",
  "simpleqa": "Knowledge",
  "socialiqa": "Reasoning",
  "squad2.0": "Understanding",
  "storycloze": "Reasoning",
  "strategyqa": "Reasoning",
  "summedits": "Language",
  "summscreen": "Understanding",
  "svamp": "Math",
  "tabmwp": "Math",
  "taco": "Code",
  "fewclue / tnews": "Understanding",
  "fewclue / bustm": "Reasoning",
  "fewclue / csl": "Understanding",
  "triviaqa": "Knowledge",
  "triviaqa-rc": "Knowledge",
  "truthfulqa": "Safety",
  "tydi-qa": "Language",
  "superglue / wic": "Language",
  "superglue / wsc": "Language",
  "winogrande": "Language",
  "xcopa": "Language",
  "xiezhi": "Knowledge",
  "xlsum": "Understanding",
  "xsum": "Understanding",
  "glue / cola": "Understanding",
  "glue / mprc": "Understanding",
  "glue / qqp": "Understanding",
  "omni-math": "Math",
  "wikibench": "Knowledge",
  "supergpqa": "Knowledge",
  "climaqa": "Science",
  "physics": "Science",
  "smolinstruct": "Science",
  "sciknoweval": "Science",
  "internsandbox": "Reasoning",
  "nejmaibench": "Science",
  "medbullets": "Science",
  "medmcqa": "Science",
  "phybench": "Science",
  "beyondaime": "Math",
  "eese": "Science"
}