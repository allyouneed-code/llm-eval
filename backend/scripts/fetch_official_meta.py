import requests
import json
import os
from bs4 import BeautifulSoup

# OpenCompass å®˜æ–¹æ–‡æ¡£çš„æ•°æ®é›†ç»Ÿè®¡é¡µé¢
# å¦‚æœä½ æœ‰ç‰¹å®šçš„æœ¬åœ° HTML æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥æ”¹ç”¨ open() è¯»å–
TARGET_URL = "https://doc.opencompass.org.cn/dataset_statistics.html"
# å¤‡ç”¨è‹±æ–‡ç‰ˆ: "https://opencompass.readthedocs.io/en/latest/dataset_statistics.html"

OUTPUT_FILE = os.path.join(os.path.dirname(__file__), "dataset_capabilities.json")

def fetch_and_parse():
    print(f"ğŸ•µï¸  æ­£åœ¨æŠ“å–é¡µé¢: {TARGET_URL} ...")
    
    try:
        # 1. è·å–ç½‘é¡µå†…å®¹
        resp = requests.get(TARGET_URL, timeout=10)
        resp.raise_for_status()
        html_content = resp.text
        
        # 2. è§£æ HTML
        soup = BeautifulSoup(html_content, "html.parser")
        
        # 3. å¯»æ‰¾åŒ…å«æ•°æ®é›†åˆ—è¡¨çš„è¡¨æ ¼
        # é€šå¸¸æ˜¯é¡µé¢ä¸­å«æœ‰ "Supported Dataset List" æ ‡é¢˜ä¸‹çš„ç¬¬ä¸€ä¸ªè¡¨æ ¼
        mapping = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼ï¼Œéå†å¯»æ‰¾åŒ…å« "Category" è¡¨å¤´çš„é‚£ä¸ª
        tables = soup.find_all("table")
        target_table = None
        
        for table in tables:
            headers = [th.get_text(strip=True).lower() for th in table.find_all("th")]
            if "name" in headers and "category" in headers:
                target_table = table
                break
        
        if not target_table:
            print("âŒ æœªæ‰¾åˆ°åŒ…å« Name å’Œ Category çš„è¡¨æ ¼ï¼Œé¡µé¢ç»“æ„å¯èƒ½å·²å˜æ›´ã€‚")
            return

        # 4. æå–æ•°æ®
        # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯ Nameï¼Œç¬¬äºŒåˆ—æ˜¯ Category (æ ¹æ®æ–‡æ¡£ç»“æ„)
        rows = target_table.find_all("tr")
        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 2:
                # æå–æ•°æ®é›†åç§° (ç§»é™¤å¤šä½™ç©ºæ ¼)
                name = cols[0].get_text(strip=True)
                category = cols[1].get_text(strip=True)
                
                # æ¸…æ´— Category (æœ‰äº›å¯èƒ½åŒ…å«æ–œæ  "Knowledge / Law")
                # æˆ‘ä»¬åªå–ç¬¬ä¸€ä¸ªä¸»è¦åˆ†ç±»ï¼Œæˆ–è€…ä¿ç•™åŸæ ·
                main_category = category.split('/')[0].strip()
                
                # å»ºç«‹æ˜ å°„: åå­— -> èƒ½åŠ›
                # åŒæ—¶å­˜å°å†™é”®ï¼Œæ–¹ä¾¿åç»­ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
                mapping[name.lower()] = main_category
                
                # éƒ¨åˆ†æ•°æ®é›†å¯èƒ½æœ‰åˆ«åï¼Œè¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦åšç‰¹æ®Šå¤„ç†
        
        print(f"âœ… è§£ææˆåŠŸï¼å…±è·å– {len(mapping)} ä¸ªæ•°æ®é›†çš„åˆ†ç±»ä¿¡æ¯ã€‚")
        
        # 5. ä¿å­˜ç»“æœ
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(mapping, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å·²ä¿å­˜è‡³: {OUTPUT_FILE}")
        
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    # éœ€è¦å®‰è£…ä¾èµ–: pip install requests beautifulsoup4
    fetch_and_parse()